---
title: "Predicting Gentrification in Philadelphia"
author: "Akira Di Sandro & Benjamin Myers"
date: "2023-12-15"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---
# Memo 
From: PHL Geo Inc.
TO: City Council
Subject: Introduction of a new development monitoring tool

Dear City Council Members, 

We are PHL Geo Inc., a Philadelphia based civil-service oriented coding organization using the power of geospatial analysis and city-owned data to provide informed insights to decision makers such as yourself. We are reaching out to introduce an exciting new tool which can be used in your districts immediately. The tool is called the GENTRISK and uses 10 years of development information to predict where development may happen in your community in the near future. The GENTRISK may be used in your district to manage the negative impacts of development, including trash dumping, street closures, and noise complaints. Getting ahead of development impacts can improve coordination and communication with residents in your district, improve the likelihood of successful permit applications, and enable smart zoning and growth in your city council region. 

The GENTRISK uses information from the past 10 years of development and combines that with external factors including the distance to schools, tree density, and distance to public transit, in order to understand what is driving development in Philadelphia. The GENTRISK then uses the relationships between those factors to predict where development may occur so you can get ahead of any potential negative impacts. These predictions form the basis of several tools avialable in GENTRSIK, such as scenario planning. In the scenario planning framework, you may modify inputs to see how they will impact development going forward. For example, if the new Philly Tree Plan is likely to bring more tree canopy to your community, you can use the GENTRISK to model how much further development is likely to occur. 

The following document provides a technical overview of how the GENTRISK functions, how to interpret its results, and how it can be incorporated into your city planning techniques. We look forward to seeing GENTRISK integrated into your city planning toolkit. 

For questions, comments, concerns, or to learn more, please email phlgeo@gmail.com

Kindly, 
Akira DiSandro
Benjamin Myers

PHL GEO Inc. Founders

# Technical Details

The GENTRISK tool uses the construction of new housing and commerical buildings as its core output, as construction provides both opportunities (in the form of economic development and increased tax base) and pressures (in the form of gentrification and noise/trash).


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(sf)
library(tidyverse)
library(tidycensus)
library(viridis)
library(gridExtra)
library(ggpubr)
library(scales)
library(ggcorrplot)
library(spdep)
library(FNN)
library(kableExtra)
library(spatstat.explore)
library(raster)
library(classInt)
#devtools::install_github("CityOfPhiladelphia/rphl")
library(rphl)

set.seed(172)

#setwd("~/Documents/MUSA5080")

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

```

```{r data download + wrangling, results='hide'}
# philly permit data
dat_permit <- st_read("https://phl.carto.com/api/v2/sql?q=SELECT+*+FROM+permits&filename=permits&format=geojson&skipfields=cartodb_id")
#
# # only keep new construction permits
newcon_permits <- dat_permit %>%
  filter(grepl("NEW CON|NEWCON",typeofwork)) %>%
  st_transform(crs = 2272)

# TODO:
# need to further limit this to permits between 2011-2021, to then predict 2022 data

# write out smaller geojson file
# st_write(newcon_permits,"Assignments/HW07-Final/data/newcon_permits.geojson")

# rm(dat_permit)

# newcon_permits <- st_read("Assignments/HW07-Final/data/newcon_permits.geojson") #%>%
#   st_transform(crs = 2272)
# for some reason, points are coming out as empty from this geojson
# this timee it worked?? idk what's going on

# census data
# variables of interest:
# B19013_001 - medHHincome
# B25026_001 - total pop 
# B02001_002 - white pop
# med home value (if possible)
# B25058_001 - median rent
# B15003_022 - attainment of bachelor's of population 25+
acs_variable_list.2021 <- load_variables(2021, #year
                                         "acs5", #five year ACS estimates
                                         cache = TRUE)

census_vars <- c("B01001_001E","B15003_022E","B19013_001E","B02001_002E","B25058_001E") # census variables of interest 
tracts21 <- 
  get_acs(geography = "block group", 
          variables = census_vars, 
          year = 2021, state = 42,
          geometry = T, output = "wide") %>%
  st_transform(crs = 2272) %>%
  dplyr::select(!matches("M$")) %>% 
  rename(total_pop = B01001_001E,
         white_pop = B02001_002E,
         bachelors25 = B15003_022E,
         med_hh_inc = B19013_001E,
         med_rent = B25058_001E) %>%
  mutate(pct_white = ifelse(total_pop > 0, white_pop / total_pop,0),
         RaceContext = ifelse(pct_white > 0.5, "Majority White", 
                              ifelse(total_pop != 0 , "Majority non-White", NA)),
         year = "2021")

# philly neighborhood data
nhoods_path <- 'https://raw.githubusercontent.com/azavea/geo-data/master/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.geojson'
nhoods <- st_read(nhoods_path, quiet = T) %>%
  st_transform(crs = 2272) %>%
  dplyr::select(mapname)

# philly bounds
philly <- st_read("https://opendata.arcgis.com/datasets/405ec3da942d4e20869d4e1449a2be48_0.geojson") %>%
  st_transform(crs = 2272) %>% 
  dplyr::select(OBJECTID,geometry)

# vacant lots/buildings
vacant_centroids <- st_read("https://opendata.arcgis.com/datasets/b990222a527849229b4192feb4c42dc0_0.geojson") %>% 
  st_transform(crs = 2272) %>% 
  st_centroid() %>%
  mutate(Legend = "Vacants") %>%
  dplyr::select(Legend)

# parks & rec
ppr_sites <- st_read("https://opendata.arcgis.com/api/v3/datasets/9eb26a787a6e448ba426eea7f9f0d93a_0/downloads/data?format=geojson&spatialRefId=4326") %>% 
  st_transform(crs = 2272) %>% 
  mutate(Legend = "Parks and Rec") %>%
  dplyr::select(Legend)

# transit stops
el <- st_read("https://opendata.arcgis.com/datasets/8c6e2575c8ad46eb887e6bb35825e1a6_0.geojson") %>% 
  st_transform(crs = 2272)
bsl <- st_read("https://opendata.arcgis.com/datasets/2e9037fd5bef406488ffe5bb67d21312_0.geojson") %>% 
  st_transform(crs = 2272)

septaStops <-
  rbind(
    el %>%
      mutate(Line = "El") %>%
      dplyr::select(Station, Line),
    bsl %>%
      mutate(Line ="Broad_St") %>%
      dplyr::select(Station, Line)) %>%
  st_transform(crs = 2272) %>%
  mutate(Legend = "Subway Stops") %>%
  dplyr::select(Legend)

# proximity to CBD using city_hall as proxy
city_hall <- bsl %>%
  st_transform(crs = 2272) %>%
  filter(Station=="City Hall") %>%
  mutate(Legend = "City Hall") %>%
  dplyr::select(Legend)

# schools
schools <- st_read('https://opendata.arcgis.com/datasets/d46a7e59e2c246c891fbee778759717e_0.geojson') %>% 
  st_transform(crs = 2272) %>% 
  mutate(Legend = "Schools") %>% 
  dplyr::select(Legend)

# trees
trees <- st_read("https://opendata.arcgis.com/api/v3/datasets/30ef36e9e880468fa74e2d5b18da4cfb_0/downloads/data?format=geojson&spatialRefId=4326") %>%
  st_transform(crs = 2272) %>% 
  mutate(Legend = "Trees") %>% 
  dplyr::select(Legend)

# grocery stores (last updated 2019)
# this data is already in a count per block group format
groshies <- st_read("https://opendata.arcgis.com/datasets/53b8a1c653a74c92b2de23a5d7bf04a0_0.geojson") %>%
  st_transform(crs = 2272) #%>%
  # dplyr::select(TOTAL_HPSS,TOTAL_RESTAURANTS)
# TOTAL_HPSS = Total number of high-produce supply stores within a half mile walking distance of the block group

# bike info
# use as count or binary data (does this square in the fishnet have a street that is bike-accessible in it?)
bikes <- st_read("https://opendata.arcgis.com/datasets/b5f660b9f0f44ced915995b6d49f6385_0.geojson") %>%
  st_transform(crs = 2272) %>%
  mutate(Legend = "Bike Network") #%>%
  # dplyr::select(Legend)

# historic districts
# use as binary (is the centroid of the fishnet square in one of these historic districts?)
historicDist <- st_read("https://phl.carto.com/api/v2/sql?q=SELECT+*+FROM+historicdistricts_local&filename=historicdistricts_local&format=geojson&skipfields=cartodb_id") %>%
  st_transform(crs = 2272) %>%
  dplyr::select(name)
```

## Introduction

### Background



# IDEA! it would be nice if we could have an animated plot demonstrating how permit count changed every year + another one of how neighborhood haverages of medHHincome chnaged over those same years. maybe even put on the same map if possible?


## Methods

### The Fishnet



#### Figure 2. Vandalism on Fishnet

```{r fig2/fishnet outcome}
# our CRS is in feet, but want to make fishnet a 500m x 500m
cell_size <- 500 * 3.28084

fishnet <- 
  st_make_grid(philly,
               cellsize = cell_size,  
               square = TRUE,
               crs = 2272) %>% 
  .[philly] %>%
  st_sf() %>%              
  mutate(uniqueID = 1:n()) 

permit_net <- 
  dplyr::select(newcon_permits) %>% 
  mutate(count_permits = 1) %>% 
  aggregate(., fishnet, sum) %>%
  mutate(count_permits = replace_na(count_permits, 0),
         uniqueID = as.numeric(rownames(.)),
         cvID = sample(round(nrow(fishnet) / 16), size=nrow(fishnet), replace = TRUE))
#this accomplishes the cross fold validation, we need to 

ggplot() +
  geom_sf(data = permit_net, aes(fill = count_permits)) +
  scale_fill_viridis() +
  labs(title = "Count of New Construction Permits for the fishnet",
       caption = "Figure x.") +
  mapTheme()
```

```{r create vars_net}
# most vars can be added easily
# special cases: groshies,bikes,historicDist
vars_net <- rbind(vacant_centroids, ppr_sites, septaStops, city_hall, schools, trees) %>%
  st_join(fishnet, join=st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID, Legend) %>%
  summarize(count = n()) %>%
  left_join(fishnet, ., by = "uniqueID") %>%  # add geometry back in
  spread(Legend, count, fill=0) %>%  # fill in ones where fishnet was missing, count was NA with 0
  dplyr::select(-`<NA>`) %>%
  ungroup()
  
# use something like code below (from lab 6) to create nearest neighbor counts
# convenience to reduce length of function names.
st_c    <- st_coordinates
st_coid <- st_centroid

vars_net_ccoid <- st_c(st_coid(vars_net))

## create NN from abandoned cars
vars_net <- vars_net %>%
  mutate(vacant_centroids.nn = nn_function(vars_net_ccoid, st_c(vacant_centroids), 8),
         ppr_sites.nn = nn_function(vars_net_ccoid, st_c(ppr_sites), 3),
         septa_stops.nn = nn_function(vars_net_ccoid, st_c(septaStops), 2),
         city_hall.nn = nn_function(vars_net_ccoid, st_c(city_hall), 1),
         schools.nn = nn_function(vars_net_ccoid, st_c(schools), 8),
         trees.nn = nn_function(vars_net_ccoid, st_c(trees), 8)) 

# think about how to add binary variables 
# groceries, get centroids of block groups and map that on to the fishnet
# from groceries, get HPSS_ACCESS (gave up on this one because it's categorical), TOTAL_RESTAURANTS, TOTAL_HPSS
total_hpss <- st_join(groshies %>% dplyr::select(TOTAL_HPSS), fishnet) %>% 
  group_by(uniqueID) %>% 
  summarize(total_hpss = mean(TOTAL_HPSS,na.rm=T)) %>%  # take average because there might be some overlap if a fishnet encompasses centroids of multiple block groups.
  mutate(total_hpss = ifelse(is.nan(total_hpss), NA, total_hpss)) %>% 
  st_drop_geometry()

total_restaurants <- st_join(groshies %>% dplyr::select(TOTAL_RESTAURANTS), fishnet) %>% 
  group_by(uniqueID) %>% 
  summarize(total_restaurants = sum(TOTAL_RESTAURANTS,na.rm=T)) %>% 
  st_drop_geometry()

# historic - binary variable 
# find fishnet squares that intersect with historicDistricts
sqr_ishistoric <- st_intersection(fishnet,historicDist)$uniqueID # vector of uniqueIDs that intersect with a historic district

# bike network - if bike network ran through 
bike_net <- st_join(fishnet, bikes) %>%
  group_by(uniqueID) %>%
  summarise(objects = sum(OBJECTID)) %>% 
  mutate(in_bike_net = ifelse(!is.na(objects), 1,0)) %>% 
  dplyr::select(uniqueID, in_bike_net) %>%
  st_drop_geometry()

# checking the bike_net code
# ggplot() + 
#   geom_sf(data = fishnet) +
#   geom_sf(data = bikes, aes(color = CLASS)) + scale_color_gradient(low = "yellow",high = "red")

# vars_net with groshies, historic, bike_net added
vars_net1 <- cbind(vars_net, 
                     total_hpss %>% dplyr::select(-uniqueID), 
                     total_restaurants %>% dplyr::select(-uniqueID), 
                     bike_net %>% dplyr::select(-uniqueID)) %>% 
    mutate(is_historic = ifelse(uniqueID %in% sqr_ishistoric, 1, 0),
           total_hpss = ifelse(is.na(total_hpss), 0, total_hpss))

```

### Risk Factor Selection



```{r final_net}
all_net <- left_join(permit_net, st_drop_geometry(vars_net1), by="uniqueID")

# make a subset of the net with variables we're actually interested in putting in the model
# add neighborhood names to data
subset_net <- all_net %>%
  st_centroid() %>%
  st_join(dplyr::select(nhoods, mapname)) %>%
  st_drop_geometry() %>%
  left_join(dplyr::select(all_net, geometry, uniqueID), by = "uniqueID") %>%
  st_sf() %>%
  na.omit()

```

#### Figure 3. Correlation Matrix of all possible risk factors

```{r fig3/correlation matrix}
# making this moreso to see which would actually be good predictors
for_cormat <- all_net %>% 
  st_drop_geometry() %>% 
  dplyr::select(-c(uniqueID,cvID))

ggcorrplot(
  round(cor(for_cormat), 1), 
  # method = "circle",
  p.mat = cor_pmat(for_cormat),
  colors = c("#4b2875", "white", "#9c1339"),
  type="lower",
  insig = "blank",
  digits = 4,
  lab = T, lab_size = 2) +  
  labs(title = "Correlation",
       caption = "Figure x.") 

# strongest pred to keep: 
# 
```

### Spatial Process of Permit Count

Another interesting feature to add to the model is one that can help us define the spatial process of vandalism in Philadelphia. One tool to examine spatial process is local Moran's *I*, which helps us understand whether the vandalism count at a certain location is randomly distributed or clustered relative to its immediate neighbors.

### Models

Since the outcome we are interested in is a count or sum of vandalism incidents, I use a Poisson regression model to estimate the outcome. I created two types of models -- one with just risk factors as the predictors and another with both risk factors and spatial process (I ultimately only kept the model with both risk factors and spatial process).

### Validation

To assess the validity of my models I look at the accuracy (how accurate are my predictions in the set that the models are trained on?) and more importantly, generalizability (can I predict counts of vandalism across different neighborhoods? For another year?). 

1.  Accuracy

To assess accuracy of my models, I examined the mean absolute error (MAE; where error = predicted value - observed value) in the dataset containing vandalism incidents for 2021 -- the same data we used to train the model. 

2.  Generalizability

The most important validity measure of a geospatial risk prediction model is the generalizability as we want to be able to train a model and predict future outcomes, in our case counts of vandalism. I use random k-fold (with k ~ 100) cross validation as well as ‘Leave-one-group-out’ cross-validation (LOGO-CV) to assess model generalizability. LOGO-CV helps us assess model generalizability across neighborhoods. I also predict vandalism risk scores for the following year (2022) to assess whether the model generalizes across time using the 2021 kernel density.

## Results

### Visualizing Risk factors and Local Moran's *I*

[description of figure 4]

#### Figure 4. Risk Factors in final model

```{r fig4/small mult map, fig.height=15}
vars_net.long <- gather(subset_net %>% dplyr::select(-count_permits),
                        variable, value, -geometry, -uniqueID, -cvID, -mapname)

vars <- unique(vars_net.long$variable)
varList <- list()

for (i in vars) {
  varList[[i]] <- ggplot() +
      geom_sf(data = filter(vars_net.long, variable == i),aes(fill = value), colour=NA) +
      scale_fill_viridis(name = "") +
      labs(title = i) +
      mapTheme(title_size = 14) + theme(legend.position="bottom")
  }

do.call(grid.arrange,c(varList, ncol = 3, top = "Predictors of Permit Count (on fishnet)", bottom = "Figure x."))
```

[description of figure 5]

#### Figure 5. Adding indicators of significant spatial processes
```{r fig 5/small mult map local morans i, fig.height=10}
final_net.nb <- poly2nb(as_Spatial(subset_net), queen=TRUE)
final_net.weights <- nb2listw(final_net.nb, style="W", zero.policy=TRUE) # turn neighborhood weights into list of weights

local_morans <- localmoran(subset_net$count_permits, final_net.weights, zero.policy=TRUE) %>%
  as.data.frame() # Ii moran's I at ith cell, Ei expected/mean from neighbors

# join local Moran's I results to fishnet
final_net.localMorans <- 
  cbind(local_morans, as.data.frame(subset_net)) %>% 
  st_sf() %>%
  dplyr::select(`Permit Count` = count_permits, 
                `Local Morans I` = Ii, 
                `P Value` = `Pr(z != E(Ii))`) %>%
  mutate(`Significant Hotspots` = ifelse(`P Value` <= 0.001, 1, 0)) %>%
  gather(variable, value, -geometry)

# now plot
vars <- unique(final_net.localMorans$variable)
varList <- list()

for(i in vars){
  varList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(final_net.localMorans, variable == i),aes(fill = value), colour=NA) +
      scale_fill_viridis(name="") +
      labs(title=i) +
      mapTheme(title_size = 14) + theme(legend.position="bottom")
  }

do.call(grid.arrange,c(varList, ncol = 2, top = "Local Moran's I Statistics for Permit Count in Philadelphia", 
                       bottom = "Figure x."))

final_net <-
  subset_net %>% 
  mutate(permitct.isSig = 
           ifelse(localmoran(subset_net$count_permits, 
                             final_net.weights)[,5] <= 0.0000001, 1, 0)) %>%
  mutate(permitct.isSig.dist = 
           nn_function(st_coordinates(st_centroid(subset_net)),
                       st_coordinates(st_centroid(
                         filter(subset_net, permitct.isSig == 1))), 1))

```

[description of figure 6] 

#### Figure 6. Scatterplots of Risk Factors
```{r fig6/small mult scatter, fig.height=16}
correlation.long <-
  st_drop_geometry(final_net) %>%
    dplyr::select(-uniqueID, -cvID, -mapname) %>%
    gather(variable, value, -count_permits)

correlation.cor <-
  correlation.long %>%
    group_by(variable) %>%
    summarize(correlation = cor(value, count_permits, use = "complete.obs"))
    
ggplot(correlation.long, aes(value, count_permits)) +
  geom_point(size = 0.1) +
  geom_text(data = correlation.cor, aes(label = paste("r =", round(correlation, 2))),
            x=-Inf, y=Inf, vjust = 1.5, hjust = -.1) +
  geom_smooth(method = "lm", se = FALSE, colour = "black") +
  facet_wrap(~variable, ncol = 4, scales = "free") +
  labs(title = "Permit count as a function of predictors",
       caption = "Figure x.") +
  plotTheme(title_size = 14)
```

[description of figure 7]

#### Figure 7, Distribution of Vandalism count
```{r fig7/hist}
final_net %>% ggplot(aes(x = count_permits)) +
  geom_histogram(bins = 66) +
  theme_minimal() +
  labs(title = "Permit Distribution",
       x = "Count of New Construction Permits", y = "Count",
       caption = "Figure x.")
```

```{r models, results='hide'}
# just risk factors
reg.vars <- c("city_hall.nn", "in_bike_net", "is_historic", "ppr_sites.nn", "schools.nn", "septa_stops.nn", "total_hpss", "total_restaurants", "Trees", "Vacants")

## RUN REGRESSIONS
reg.CV <- crossValidate(
  dataset = final_net,
  id = "cvID",
  dependentVariable = "count_permits",
  indVariables = reg.vars) %>%
    mutate(error = count_permits - Prediction)

# MAE
reg.MAE <- mean(abs(reg.CV$error)) # 45.08406


# with local Moran's I spatial process features
reg.sp.vars <- c("city_hall.nn", "in_bike_net", "is_historic", "ppr_sites.nn", "schools.nn", "septa_stops.nn", "total_hpss", "total_restaurants", "Trees", "Vacants","permitct.isSig", "permitct.isSig.dist")

## RUN REGRESSIONS
reg.spatialCV <- crossValidate(
  dataset = final_net,
  id = "cvID",                           
  dependentVariable = "count_permits",
  indVariables = reg.sp.vars) %>% 
    dplyr::select(cvID, count_permits, Prediction, geometry) %>% 
    mutate(error = count_permits - Prediction)

# MAE
reg.spatial.MAE <- mean(abs(reg.spatialCV$error)) # 36.35607


# adding neighborhood for LOGO CV, risk factors only
reg.logoCV <- crossValidate(
  dataset = final_net,
  id = "mapname",
  dependentVariable = "count_permits",
  indVariables = reg.vars) %>%
    mutate(error = count_permits - Prediction)

# MAE
reg.logo.MAE <- mean(abs(reg.logoCV$error)) # 46.49673


# adding neighborhood for LOGO CV, risk factors + spatial process
reg.logo.spatialCV <- crossValidate(
  dataset = final_net,
  id = "mapname",                           
  dependentVariable = "count_permits",
  indVariables = reg.sp.vars) %>% 
    dplyr::select(cvID = mapname, count_permits, Prediction, geometry) %>% 
    mutate(error = count_permits - Prediction)

# MAE
reg.logo.spatial.MAE <- mean(abs(reg.logo.spatialCV$error)) # 37.12363

```

### Model Selection 

As mentioned in the Methods section, I kept both risk factors and spatial process as independent variables in my model as this model had lower values of error across all cross validation methods.

### Cross Validation

[description of figure 8 & 9]

#### Figure 8. Map of Model Errors
```{r fig8/small mult map errors}
reg.summary <- rbind(
  mutate(reg.spatialCV,
         Error = Prediction - count_permits,
         Regression = "Random k-fold CV"),
  mutate(reg.logo.spatialCV, 
         Error = Prediction - count_permits,
         Regression = "Spatial LOGO-CV")) %>%
    st_sf() 

error_by_reg_and_fold <- 
  reg.summary %>%
    group_by(Regression, cvID) %>% 
    summarize(Mean_Error = mean(Prediction - count_permits, na.rm = T),
              MAE        = mean(abs(Mean_Error), na.rm = T),
              SD_MAE     = mean(abs(Mean_Error), na.rm = T)) %>%
  ungroup()

# make map
error_by_reg_and_fold %>%
  ggplot() +
    geom_sf(aes(fill = MAE)) +
    facet_wrap(~Regression) +
    scale_fill_viridis() +
    labs(title = "Errors by Cross Validation method",
       caption = "Figure x.") +
    mapTheme(title_size = 14) + theme(legend.position="bottom")

```

#### Figure 9. Bar Plots of Error
```{r fig9/bar plots of MAE}
error_by_reg_and_fold %>%
  ggplot(aes(MAE)) + 
    geom_histogram(bins = 30, colour="black", fill = "#FDE725FF") +
    facet_wrap(~Regression) +  
    geom_vline(xintercept = 0) + scale_x_continuous(breaks = seq(0, 450, by = 50)) + 
    labs(title="Distribution of MAE", subtitle = "k-fold cross validation vs. LOGO-CV",
         x="Mean Absolute Error",     y="Count",
         caption = "Figure x.") +
    plotTheme(title_size = 14) + theme(legend.position="bottom")

```

[description of table 1]

#### Table 1. Summary of Regressions
```{r table1/MAE and SD}
st_drop_geometry(error_by_reg_and_fold) %>%
  group_by(Regression) %>% 
    summarize(`Mean MAE` = round(mean(MAE), 2),
              `SD MAE` = round(sd(MAE), 2)) %>%
  kable(caption = "Table 1: Summary of Regressions") %>%
  kable_styling("striped", full_width = F) %>% 
  kable_classic(full_width = F, html_font = "Cambria")

```

### Racial Context

In order to assess if the model generalizes to different neighborhood contexts, I specifically tested whether the model generalizes to a racial context. Using 2021 US Census data, I defined a neighborhood to be "Majority White" if over 50% of the population was White, and "Majority non-White" otherwise. 

[description of table 2]

#### Table 2. Racial Context
```{r table2/raw errors race}
RaceContext <- tracts21 %>% 
  dplyr::select(GEOID,total_pop,RaceContext,geometry) %>% 
  .[nhoods,]

reg.summary %>% 
  st_centroid() %>%
  st_join(tracts21 %>% dplyr::select(RaceContext, geometry)) %>%
  na.omit() %>%
  st_drop_geometry() %>%
  group_by(Regression, RaceContext) %>%
  summarize(mean.Error = mean(Error, na.rm = T)) %>%
  spread(RaceContext, mean.Error) %>%
  kable(caption = "Table 2. Mean Error by Neighborhood Racial Context") %>%
  kable_styling("striped", full_width = F) %>% 
  kable_classic(html_font = "Cambria")

```

#### Table 2. Income Context

# need to work on this.
```{r table2/raw errors race}
IncomeContext <- tracts22 %>% 
  dplyr::select(GEOID,total_pop,IncomeContext,geometry) %>% 
  .[nhoods,]

reg.summary %>% 
  st_centroid() %>%
  st_join(tracts22 %>% dplyr::select(IncomeContext, geometry)) %>%
  na.omit() %>%
  st_drop_geometry() %>%
  group_by(Regression, IncomeContext) %>%
  summarize(mean.Error = mean(Error, na.rm = T)) %>%
  spread(IncomeContext, mean.Error) %>%
  kable(caption = "Table 2. Mean Error by Neighborhood Income Context") %>%
  kable_styling("striped", full_width = F) %>% 
  kable_classic(html_font = "Cambria")

```

### Kernel Density and Future Predictions

Finally, I examined the kernel density of 2021 vandalism counts in Philadelphia and tested whether this model generalizes to the vandalism incident report patterns of 2022. The risk categories, from 1st in purple to 5th in yellow are ordered from least to most risk of vandalism.

[description of figure 10]

#### Figure 10. Comparison of Kernel Density and Risk Predictions
```{r fig10/kernel density map, results='hide', fig.width=10, fig.height=10}
# kernel density
vand_ppp <- as.ppp(st_coordinates(vandalism), W = st_bbox(final_net))
vand_KD.1000 <- spatstat.explore::density.ppp(vand_ppp, 1000)
vand_KD.1500 <- spatstat.explore::density.ppp(vand_ppp, 1500)
vand_KD.2000 <- spatstat.explore::density.ppp(vand_ppp, 2000)
vand_KD.df <- rbind(
  mutate(data.frame(rasterToPoints(mask(raster(vand_KD.1000), as(nhoods, 'Spatial')))), Legend = "1000 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(vand_KD.1500), as(nhoods, 'Spatial')))), Legend = "1500 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(vand_KD.2000), as(nhoods, 'Spatial')))), Legend = "2000 Ft.")) 

vand_KD.df$Legend <- factor(vand_KD.df$Legend, levels = c("1000 Ft.", "1500 Ft.", "2000 Ft."))

# ggplot(data=vand_KD.df, aes(x=x, y=y)) +
#   geom_raster(aes(fill=layer)) + 
#   facet_wrap(~Legend) +
#   coord_sf(crs=st_crs(final_net)) + 
#   scale_fill_viridis(name="Density") +
#   labs(title = "Kernel density with 3 different search radii") +
#   mapTheme(title_size = 14)


# as.data.frame(vand_KD.1000) %>%
#   st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
#   aggregate(., final_net, mean) %>%
#    ggplot() +
#      geom_sf(aes(fill=value)) +
#      geom_sf(data = sample_n(vandalism, 1500), size = .5) +
#      scale_fill_viridis(name = "Density") +
#      labs(title = "Kernel density of 2021 Vandalism Incidents") +
#      mapTheme(title_size = 14)


# download data from 2022
# incidents22 <- st_read("~/Documents/MUSA5080/Assignments/HW04/incidents_22/incidents_part1_part2.shp") %>% 
#   st_transform('ESRI:102728') %>% rename(Legend = text_gener)
# 

carto_url = "https://phl.carto.com/api/v2/sql"

# Crime incidents
table_name = "incidents_part1_part2"

# query
where2 = "dispatch_date >= '2022-01-01' AND dispatch_date < '2023-01-01' AND text_general_code IN ('DRIVING UNDER THE INFLUENCE','Theft from Vehicle','Thefts','Disorderly Conduct','Public Drunkenness', 'Arson', 'Vandalism/Criminal Mischief')"

query2 = paste("SELECT *",
              "FROM", table_name,
              "WHERE", where)

incidents22 = rphl::get_carto(query2, format = "csv", base_url = carto_url, stringsAsFactors = F)%>%
   rename(Legend = text_general_code)
incidents22 <- incidents22 %>%
   filter(!is.na(point_x) & !is.na(point_y)) %>%
   st_as_sf(coords = c("point_x","point_y"),crs=4326) %>%
     st_transform('ESRI:102728') %>%
   dplyr::select(Legend, geometry)




# filter to only keep vandalism incidents
vandalism22 <- incidents22 %>% filter(Legend == "Vandalism/Criminal Mischief") %>% 
  .[fishnet,]


# from lab
vand_KDE_sum <- as.data.frame(vand_KD.1000) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) 
kde_breaks <- classIntervals(vand_KDE_sum$value, 
                             n = 5, "fisher")
vand_KDE_sf <- vand_KDE_sum %>%
  mutate(label = "Kernel Density",
         Risk_Category = classInt::findCols(kde_breaks),
         Risk_Category = case_when(
           Risk_Category == 5 ~ "5th",
           Risk_Category == 4 ~ "4th",
           Risk_Category == 3 ~ "3rd",
           Risk_Category == 2 ~ "2nd",
           Risk_Category == 1 ~ "1st")) %>%
  cbind(
    aggregate(
      dplyr::select(vandalism22) %>% mutate(vandCount = 1), ., sum) %>%
    mutate(vandCount = replace_na(vandCount, 0))) %>%
  dplyr::select(label, Risk_Category, vandCount)


ml_breaks <- classIntervals(reg.spatialCV$Prediction, 
                             n = 5, "fisher")
vand_risk_sf <-
  reg.spatialCV %>%
  mutate(label = "Risk Predictions",
         Risk_Category =classInt::findCols(ml_breaks),
         Risk_Category = case_when(
           Risk_Category == 5 ~ "5th",
           Risk_Category == 4 ~ "4th",
           Risk_Category == 3 ~ "3rd",
           Risk_Category == 2 ~ "2nd",
           Risk_Category == 1 ~ "1st")) %>%
  cbind(
    aggregate(
      dplyr::select(vandalism22) %>% mutate(vandCount = 1), ., sum) %>%
      mutate(vandCount = replace_na(vandCount, 0))) %>%
  dplyr::select(label,Risk_Category, vandCount)


rbind(vand_KDE_sf, vand_risk_sf) %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category, -geometry) %>%
  ggplot() +
    geom_sf(aes(fill = Risk_Category), colour = NA) +
    geom_sf(data = sample_n(vandalism22, 3000), size = .25, colour = "black") +
    facet_wrap(~label, ) +
    scale_fill_viridis(discrete = TRUE) +
    labs(title="Comparison of Kernel Density and Risk Predictions",
         subtitle="2021 Vandalism; 2022 Vandalism risk predictions",
         caption = "Figure 10.") +
    theme(legend.position="bottom") +
    mapTheme(title_size = 14)

```

[description of figure 11]

#### Figure 11.
```{r fig11/bar comparison}
rbind(vand_KDE_sf, vand_risk_sf) %>%
  st_drop_geometry() %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category) %>%
  group_by(label, Risk_Category) %>%
  summarize(countVand = sum(Value)) %>%
  ungroup() %>%
  group_by(label) %>%
  mutate(Pcnt_of_test_set_crimes = countVand / sum(countVand)) %>%
    ggplot(aes(Risk_Category,Pcnt_of_test_set_crimes)) +
      geom_bar(aes(fill=label), position="dodge", stat="identity") +
      scale_fill_viridis(discrete = TRUE, name = "Model") +
      labs(title = "Risk prediction vs. Kernel density, 2022 Vandalism Incidents",
           y = "% of Test Set Vandalism Incidents (per model)",
           x = "Risk Category",
           caption = "Figure 11.") +
  theme_bw() +
      theme(axis.text.x = element_text(angle = 45, vjust = 0.5))

```

## Conclusion

In conclusion....