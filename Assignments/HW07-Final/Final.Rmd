---
title: "Predicting Development and Gentrification in Philadelphia"
author: "Akira Di Sandro & Benjamin Myers"
date: "2023-12-15"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---
# Memo 
From: PHL Geo Inc.
TO: Philadelphia City Council
Subject: Introduction of a new development monitoring tool

Dear City Council Members, 

We are PHL Geo Inc., a Philadelphia based civil-service oriented coding organization using the power of geospatial analysis and city-owned data to provide informed insights to decision makers such as yourself. We are reaching out to introduce an exciting new tool which can be used in your districts immediately. The tool is called the GENTRISK and uses 10 years of development information to predict where development may happen in your community in the near future. The GENTRISK may be used in your district to manage the negative impacts of development, including trash dumping, street closures, and noise complaints. Getting ahead of development impacts can improve coordination and communication with residents in your district, improve the likelihood of successful permit applications, and enable smart zoning and growth in your city council region. 

Development is a process determined by numerous factors and can have positive effects; however, development-induced gentrification, which is "a process a neighborhood change that includes economic change in a historically disinvested neighborhood - by means of real estate investment and new higher-income residents moving in - as well as demographic change - not only in terms of income level, but also in terms of changes in the education level or racial make-up of residents," can have negative effects [(Demsas, 2021)]("https://www.vox.com/22629826/gentrification-definition-housing-racism-segregation-cities"). As city council members looking to promote econoimc development while ensuring that residents have the protections that they need, the GENTRISK can provide tools that will help you see where development pressures are happening and engage in proactive outreach to those communities. Aligning city services such as increaseing access to Philadelhpia's Homestead Exemption or to the [PA Homeowner Assistance fund]("https://pahaf.org/"), can reduce these negative impacts. 

The GENTRISK uses information from the past 10 years of development and combines that with external factors including the distance to schools, tree density, and distance to public transit, in order to understand what is driving development in Philadelphia. The GENTRISK then uses the relationships between those factors to predict where development may occur so you can get ahead of any potential negative impacts. These predictions form the basis of several tools available in GENTRISK, such as scenario planning. In the scenario planning framework, you may modify inputs to see how they will impact development going forward. For example, if the new Philly Tree Plan is likely to bring more tree canopy to your community, you can use the GENTRISK to model how much further development is likely to occur. 

The following document provides a technical overview of how the GENTRISK functions, how to interpret its results, and how it can be incorporated into your city planning techniques. We look forward to seeing GENTRISK integrated into your city planning toolkit. 

For questions, comments, concerns, or to learn more, please email phlgeo@gmail.com

Kindly, 
Akira DiSandro
Benjamin Myers

PHL GEO Inc. Founders

# Technical Details

The GENTRISK tool uses the construction of new housing and commercial buildings as its core output, as construction provides both opportunities (in the form of economic development and increased tax base) and pressures (in the form of gentrification and noise/trash) on residents and neighborhoods. Construction information is available from the [Department of Licenses and Inspection]("https://data.phila.gov/visualizations/li-building-permits") for a time period stretching over the last 10 years. While neighborhoods are constantly in a state of change, focusing in on a more specific time period can provide City Council with an accurate understanding of what is driving development in the city. THe time period from 2013 to 2018 provides a good baseline of understanding development drivers as it is prior to the impacts that the COVID-19 global pandemic had on development drivers, yet recent enough that certain key drivers - such as the turnover of permanent residents for renters - are still prevalent within Philadelphia. 

```{r setup, include=FALSE, results='hide'}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(sf)
library(tidyverse)
library(tidycensus)
library(viridis)
library(gridExtra)
library(ggpubr)
library(scales)
library(ggcorrplot)
library(spdep)
library(FNN)
library(kableExtra)
library(spatstat.explore)
library(raster)
library(classInt)
#devtools::install_github("CityOfPhiladelphia/rphl")
library(rphl)
library(stringr)
library(lubridate)

set.seed(172)

#setwd("~/Documents/MUSA5080")

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

```

# Data Exploration

The following code pulls the construction permit information for the City and brings in a number of external variables. In the following section, the variables are loaded into the model, which include the following:

- Vacant and abandoned housing
- Philadelphia Parks and Recreation Locations
- Septa Stops
- Distance to the 3 closest schools
- Median Household Income
- Renter Occupied Housing
- Owner-occupied
- Private vehicle ownership
- Dangerous Sidewalk complaints
- Illegal dumping complaints
- Parks and Recreation Safety and Maintenance complaints
- Rubbish / Recyclable Material Collection complaints
- Street Tree maintenance complaints
- And the distance between those variables to eachother. 



```{r data download + wrangling, results='hide'}
# philly permit data
dat_permit <- st_read("https://phl.carto.com/api/v2/sql?q=SELECT+*+FROM+permits&filename=permits&format=geojson&skipfields=cartodb_id")

# only keep new construction permits
newcon_permits <- dat_permit %>%
  filter(grepl("NEW CON|NEWCON",typeofwork)) %>%
  mutate(year = substr(permitissuedate, 1,4)) %>%
  filter(year %in% c(2013:2019,2022)) %>%
  st_transform(crs = 2272)

# census data
acs_variable_list.2019 <- load_variables(2019, #year
                                         "acs5", #five year ACS estimates
                                         cache = TRUE) %>% 
  filter(geography == "block group")
    
# variables of interest:
# B19013_001: medHHincome
# B01001_001: total pop 
# B02001_002: white pop
# B25058_001: median rent
# B15003_022: attainment of bachelor's of population 25+
# B25071_001: Median gross rent as a percentage of household income (past year)
# B07201_001: Geographical Mobility in the Past Year for Current Residence--Metropolitan Statistical Area Level in the United States, estimated total
# B07201_002: Geographical Mobility in the Past Year for Current Residence--Metropolitan Statistical Area Level in the United States, same house 1 year ago 
# B25008_002: total population in occupied housing by tenure, owner occupied
# B25008_003: total population in occupied housing by tenure, renter occupied
# B99082_001: allocation of private vehicle occupancy
# B25044_001: tenure by vehicles available
# B09002_001: own children under 18
# B11004_001: related children under 18

census_vars <- paste0(c("B01001_001", "B19013_001", "B02001_002", "B25058_001", "B15003_022", "B25071_001", "B07201_001",
                        "B07201_002", "B25008_002", "B25008_003", "B99082_001", "B25044_001", "B09002_001", 
                        "B11004_001"),"E") # census variables of interest that are available
    
medHHinc2013 <- 78986 # source: https://www.deptofnumbers.com/income/pennsylvania/philadelphia/
medHHinc2014 <- 76334
medHHinc2015 <- 75790
medHHinc2016 <- 74512
medHHinc2017 <- 74474
medHHinc2018 <- 71221
medHHinc2019 <- 70459

tracts13 <- 
  get_acs(geography = "block group", 
          variables = census_vars, 
          year = 2013, state = 42,
          geometry = T, output = "wide") %>%
  st_transform(crs = 2272) %>%
  dplyr::select(!matches("M$")) %>% 
  rename(total_pop = B01001_001E,
         med_hh_inc = B19013_001E,
         white_pop = B02001_002E,
         med_rent = B25058_001E,
         bachelors25 = B15003_022E,
         pct_rent_hhinc = B25071_001E,
         mobility_tot_metro = B07201_001E,
         samehouse1yr_metro = B07201_002E,
         owner_occ = B25008_002E,
         renter_occ = B25008_003E,
         private_vehicle_occ = B99082_001E,
         vehicles_avail = B25044_001E,
  ) %>%
  mutate(children = B09002_001E + B11004_001E,
         pct_white = ifelse(total_pop > 0, white_pop / total_pop,0),
         RaceContext = ifelse(pct_white > 0.5, "Majority White", 
                              ifelse(total_pop != 0 , "Majority non-White", NA)),
         year = "2013") %>% 
  dplyr::select(!matches("^B|white_pop")) %>% 
  st_centroid()

tracts14 <- 
  get_acs(geography = "block group", 
          variables = census_vars, 
          year = 2014, state = 42,
          geometry = T, output = "wide") %>%
  st_transform(crs = 2272) %>%
  dplyr::select(!matches("M$")) %>% 
  rename(total_pop = B01001_001E,
         med_hh_inc = B19013_001E,
         white_pop = B02001_002E,
         med_rent = B25058_001E,
         bachelors25 = B15003_022E,
         pct_rent_hhinc = B25071_001E,
         mobility_tot_metro = B07201_001E,
         samehouse1yr_metro = B07201_002E,
         owner_occ = B25008_002E,
         renter_occ = B25008_003E,
         private_vehicle_occ = B99082_001E,
         vehicles_avail = B25044_001E,
  ) %>%
  mutate(children = B09002_001E + B11004_001E,
         pct_white = ifelse(total_pop > 0, white_pop / total_pop,0),
         RaceContext = ifelse(pct_white > 0.5, "Majority White", 
                              ifelse(total_pop != 0 , "Majority non-White", NA)),
         year = "2014") %>% 
  dplyr::select(!matches("^B|white_pop")) %>% 
  st_centroid()

tracts15 <- 
  get_acs(geography = "block group", 
          variables = census_vars, 
          year = 2015, state = 42,
          geometry = T, output = "wide") %>%
  st_transform(crs = 2272) %>%
  dplyr::select(!matches("M$")) %>% 
  rename(total_pop = B01001_001E,
         med_hh_inc = B19013_001E,
         white_pop = B02001_002E,
         med_rent = B25058_001E,
         bachelors25 = B15003_022E,
         pct_rent_hhinc = B25071_001E,
         mobility_tot_metro = B07201_001E,
         samehouse1yr_metro = B07201_002E,
         owner_occ = B25008_002E,
         renter_occ = B25008_003E,
         private_vehicle_occ = B99082_001E,
         vehicles_avail = B25044_001E,
  ) %>%
  mutate(children = B09002_001E + B11004_001E,
         pct_white = ifelse(total_pop > 0, white_pop / total_pop,0),
         RaceContext = ifelse(pct_white > 0.5, "Majority White", 
                              ifelse(total_pop != 0 , "Majority non-White", NA)),
         year = "2015") %>% 
  dplyr::select(!matches("^B|white_pop")) %>% 
  st_centroid()

tracts16 <- 
  get_acs(geography = "block group", 
          variables = census_vars, 
          year = 2016, state = 42,
          geometry = T, output = "wide") %>%
  st_transform(crs = 2272) %>%
  dplyr::select(!matches("M$")) %>% 
  rename(total_pop = B01001_001E,
         med_hh_inc = B19013_001E,
         white_pop = B02001_002E,
         med_rent = B25058_001E,
         bachelors25 = B15003_022E,
         pct_rent_hhinc = B25071_001E,
         mobility_tot_metro = B07201_001E,
         samehouse1yr_metro = B07201_002E,
         owner_occ = B25008_002E,
         renter_occ = B25008_003E,
         private_vehicle_occ = B99082_001E,
         vehicles_avail = B25044_001E,
  ) %>%
  mutate(children = B09002_001E + B11004_001E,
         pct_white = ifelse(total_pop > 0, white_pop / total_pop,0),
         RaceContext = ifelse(pct_white > 0.5, "Majority White", 
                              ifelse(total_pop != 0 , "Majority non-White", NA)),
         year = "2016") %>% 
  dplyr::select(!matches("^B|white_pop")) %>% 
  st_centroid()

tracts17 <- 
  get_acs(geography = "block group", 
          variables = census_vars, 
          year = 2017, state = 42,
          geometry = T, output = "wide") %>%
  st_transform(crs = 2272) %>%
  dplyr::select(!matches("M$")) %>% 
  rename(total_pop = B01001_001E,
         med_hh_inc = B19013_001E,
         white_pop = B02001_002E,
         med_rent = B25058_001E,
         bachelors25 = B15003_022E,
         pct_rent_hhinc = B25071_001E,
         mobility_tot_metro = B07201_001E,
         samehouse1yr_metro = B07201_002E,
         owner_occ = B25008_002E,
         renter_occ = B25008_003E,
         private_vehicle_occ = B99082_001E,
         vehicles_avail = B25044_001E,
  ) %>%
  mutate(children = B09002_001E + B11004_001E,
         pct_white = ifelse(total_pop > 0, white_pop / total_pop,0),
         RaceContext = ifelse(pct_white > 0.5, "Majority White", 
                              ifelse(total_pop != 0 , "Majority non-White", NA)),
         year = "2017") %>% 
  dplyr::select(!matches("^B|white_pop")) %>% 
  st_centroid()

tracts18 <- 
  get_acs(geography = "block group", 
          variables = census_vars, 
          year = 2018, state = 42,
          geometry = T, output = "wide") %>%
  st_transform(crs = 2272) %>%
  dplyr::select(!matches("M$")) %>% 
  rename(total_pop = B01001_001E,
         med_hh_inc = B19013_001E,
         white_pop = B02001_002E,
         med_rent = B25058_001E,
         bachelors25 = B15003_022E,
         pct_rent_hhinc = B25071_001E,
         mobility_tot_metro = B07201_001E,
         samehouse1yr_metro = B07201_002E,
         owner_occ = B25008_002E,
         renter_occ = B25008_003E,
         private_vehicle_occ = B99082_001E,
         vehicles_avail = B25044_001E,
  ) %>%
  mutate(children = B09002_001E + B11004_001E,
         pct_white = ifelse(total_pop > 0, white_pop / total_pop,0),
         RaceContext = ifelse(pct_white > 0.5, "Majority White", 
                              ifelse(total_pop != 0 , "Majority non-White", NA)),
         year = "2018") %>% 
  dplyr::select(!matches("^B|white_pop")) %>% 
  st_centroid()

tracts19 <- 
  get_acs(geography = "block group", 
          variables = census_vars, 
          year = 2019, state = 42,
          geometry = T, output = "wide") %>%
  st_transform(crs = 2272) %>%
  dplyr::select(!matches("M$")) %>% 
  rename(total_pop = B01001_001E,
         med_hh_inc = B19013_001E,
         white_pop = B02001_002E,
         med_rent = B25058_001E,
         bachelors25 = B15003_022E,
         pct_rent_hhinc = B25071_001E,
         mobility_tot_metro = B07201_001E,
         samehouse1yr_metro = B07201_002E,
         owner_occ = B25008_002E,
         renter_occ = B25008_003E,
         private_vehicle_occ = B99082_001E,
         vehicles_avail = B25044_001E,
  ) %>%
  mutate(children = B09002_001E + B11004_001E,
         pct_white = ifelse(total_pop > 0, white_pop / total_pop,0),
         RaceContext = ifelse(pct_white > 0.5, "Majority White", 
                              ifelse(total_pop != 0 , "Majority non-White", NA)),
         year = "2019") %>% 
  dplyr::select(!matches("^B|white_pop")) %>% 
  st_centroid()

# philly neighborhood data
nhoods_path <- 'https://raw.githubusercontent.com/azavea/geo-data/master/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.geojson'
nhoods <- st_read(nhoods_path, quiet = T) %>%
  st_transform(crs = 2272) %>%
  dplyr::select(mapname)

# philly bounds
philly <- st_read("https://opendata.arcgis.com/datasets/405ec3da942d4e20869d4e1449a2be48_0.geojson") %>%
  st_transform(crs = 2272) %>% 
  dplyr::select(OBJECTID,geometry)

# limit permit dat to philly border
newcon_permits <- newcon_permits %>% 
  .[philly,]

# 311 incident reports data
carto_url = "https://phl.carto.com/api/v2/sql"

# Crime incidents
table_name_311 = "public_cases_fc"

# query
where_311 = "closed_datetime >= '2010-01-01' AND closed_datetime < '2020-01-01' AND service_name IN ('Rubbish/Recyclable Material Collection','Illegal Dumping','Construction Complaints', 'Building Construction', 'Sanitation Violation', 'Street Trees', 'Dangerous Sidewalk', 'Homeless Encampment Request', 'Parks and Rec Safety and Maintenance', 'Vacant House or Commercial')"

query311 = paste("SELECT *",
                 "FROM", table_name_311,
                 "WHERE", where_311)

reports311 = rphl::get_carto(query311, format = "csv", base_url = carto_url, stringsAsFactors = F)
reports311 <- reports311 %>%
  mutate(Year = year(format.Date(reports311$closed_datetime)))
reports311 <- reports311 %>%
  filter(!is.na(lon) & !is.na(lat)) %>%
  st_as_sf(coords = c("lon","lat"), crs = 4326) %>%
  st_transform(crs = 2272) %>% 
  mutate(
    type = service_name,
    Legend = "311") %>% 
  dplyr::select(Legend, type, geometry, Year)
  
  
```

Development patterns may be spread over space, such that it forms a smooth 'mosaic' across the city of Philadelphia. Alternatively, it is possible that several permits may be clustered together, either because they were all submitted as part of the same construction project (ie, submitting an electrical permit and roofing permit for the same property) or there were several units being worked in tandem. To better understand whether Philadelphia is experiencing a 'mosaic' of development or if development has been clustered into particular areas, a fishnet was established for grid cells of 500 meter by 500 meters across the city. The 500 meter scale (1/3rd mile) allows for a fluid understanding of development patterns at the block scale. 


Figure 1 below shows the number of permits granted for each fishnet square in 2014, followed by Figure 2 showing the number of permits for each fishnet square in 2015, and so on until Figure 6 demonstrating the number of permits granted for each fishnet square in 2019. As is visible through the maps, trends and patterns of development shift across the city, such that in 2014 development centered around the Center City Neighborhood, while moving towards 2019 - when record number of permits were granted for the time period in focus - development was centered in Center City as well as North Philadelphia. 

#### Figure 2. Permit Count on Fishnet

```{r fig2/fishnet outcome}
# our CRS is in feet, but want to make fishnet a 500m x 500m
cell_size <- 500 * 3.28084

fishnet <- 
  st_make_grid(philly,
               cellsize = cell_size,  
               square = TRUE,
               crs = 2272) %>% 
  .[philly] %>%
  st_sf() %>%              
  mutate(uniqueID = 1:n()) 

# 2013 
{
  permit_net13 <- 
    dplyr::select(newcon_permits %>% filter(year == "2013")) %>% 
    mutate(count_permits = 1) %>% 
    aggregate(., fishnet, sum) %>%
    mutate(count_permits13 = replace_na(count_permits, 0),
           uniqueID = as.numeric(rownames(.)),
           cvID = sample(round(nrow(fishnet) / 16), size=nrow(fishnet), replace = TRUE))
}

# 2014
{
  permit_net14 <- 
    dplyr::select(newcon_permits %>% filter(year == "2014")) %>% 
    mutate(count_permits = 1) %>% 
    aggregate(., fishnet, sum) %>%
    mutate(count_permits14 = replace_na(count_permits, 0),
           uniqueID = as.numeric(rownames(.)),
           cvID = sample(round(nrow(fishnet) / 16), size=nrow(fishnet), replace = TRUE)) %>% 
    dplyr::select(-count_permits)
}

# 2015
{
  permit_net15 <- 
    dplyr::select(newcon_permits %>% filter(year == "2015")) %>% 
    mutate(count_permits = 1) %>% 
    aggregate(., fishnet, sum) %>%
    mutate(count_permits15 = replace_na(count_permits, 0),
           uniqueID = as.numeric(rownames(.)),
           cvID = sample(round(nrow(fishnet) / 16), size=nrow(fishnet), replace = TRUE)) %>% 
    dplyr::select(-count_permits)
}

# 2016
{
  permit_net16 <- 
    dplyr::select(newcon_permits %>% filter(year == "2016")) %>% 
    mutate(count_permits = 1) %>% 
    aggregate(., fishnet, sum) %>%
    mutate(count_permits16 = replace_na(count_permits, 0),
           uniqueID = as.numeric(rownames(.)),
           cvID = sample(round(nrow(fishnet) / 16), size=nrow(fishnet), replace = TRUE)) %>% 
    dplyr::select(-count_permits)
}

# 2017
{
  permit_net17 <- 
    dplyr::select(newcon_permits %>% filter(year == "2017")) %>% 
    mutate(count_permits = 1) %>% 
    aggregate(., fishnet, sum) %>%
    mutate(count_permits17 = replace_na(count_permits, 0),
           uniqueID = as.numeric(rownames(.)),
           cvID = sample(round(nrow(fishnet) / 16), size=nrow(fishnet), replace = TRUE)) %>% 
    dplyr::select(-count_permits)
}

# 2018
{
  permit_net18 <- 
    dplyr::select(newcon_permits %>% filter(year == "2018")) %>% 
    mutate(count_permits = 1) %>% 
    aggregate(., fishnet, sum) %>%
    mutate(count_permits18 = replace_na(count_permits, 0),
           uniqueID = as.numeric(rownames(.)),
           cvID = sample(round(nrow(fishnet) / 16), size=nrow(fishnet), replace = TRUE)) %>% 
    dplyr::select(-count_permits)
}

# 2019
{
  permit_net19 <- 
    dplyr::select(newcon_permits %>% filter(year == "2019")) %>% 
    mutate(count_permits = 1) %>% 
    aggregate(., fishnet, sum) %>%
    mutate(count_permits19 = replace_na(count_permits, 0),
           uniqueID = as.numeric(rownames(.)),
           cvID = sample(round(nrow(fishnet) / 16), size=nrow(fishnet), replace = TRUE)) %>% 
    dplyr::select(-count_permits)
}

# combine permit counts from all years
permit_net_allyrs <- permit_net13 %>% 
  st_drop_geometry() %>%
  dplyr::select(uniqueID,cvID,count_permits13) %>% 
  left_join(permit_net14 %>% st_drop_geometry() %>% dplyr::select(uniqueID,count_permits14), by = "uniqueID") %>% 
  left_join(permit_net15 %>% st_drop_geometry() %>% dplyr::select(uniqueID,count_permits15), by = "uniqueID") %>% 
  left_join(permit_net16 %>% st_drop_geometry() %>% dplyr::select(uniqueID,count_permits16), by = "uniqueID") %>% 
  left_join(permit_net17 %>% st_drop_geometry() %>% dplyr::select(uniqueID,count_permits17), by = "uniqueID") %>% 
  left_join(permit_net18 %>% st_drop_geometry() %>% dplyr::select(uniqueID,count_permits18), by = "uniqueID") %>% 
  left_join(permit_net19 %>% st_drop_geometry() %>% dplyr::select(uniqueID,count_permits19), by = "uniqueID") %>% 
  left_join(permit_net13 %>% dplyr::select(-c(cvID,count_permits13,count_permits)), by = "uniqueID")

# TODO: edit this to show permit count for all years on separate maps
# would need to pur permit_net_allyrs into long form so i can use facet_wrap()
# for '13, '15, '17, '19

# permit_count_map <- ggplot() +
#   geom_sf(data = permit_net_allyrs, aes(fill = count_permits)) +
#   scale_fill_viridis() +
#   labs(title = "Count of New Construction Permits for the fishnet",
#        caption = "Figure x.") +
#   mapTheme()
# 
# permit_count_map

 permit_count_map14 <- ggplot() +
   geom_sf(data = permit_net14, aes(fill = count_permits14)) +
   scale_fill_viridis() +
   labs(title = "Count of New Construction Permits for the fishnet 2014",
        caption = "Figure 1.") +
   mapTheme()

 permit_count_map15 <- ggplot() +
   geom_sf(data = permit_net15, aes(fill = count_permits15)) +
   scale_fill_viridis() +
   labs(title = "Count of New Construction Permits for the fishnet 2015",
        caption = "Figure 2.") +
   mapTheme()

 permit_count_map16 <- ggplot() +
   geom_sf(data = permit_net16, aes(fill = count_permits16)) +
   scale_fill_viridis() +
   labs(title = "Count of New Construction Permits for the fishnet 2016",
        caption = "Figure 3.") +
   mapTheme()

 permit_count_map17 <- ggplot() +
   geom_sf(data = permit_net17, aes(fill = count_permits17)) +
   scale_fill_viridis() +
   labs(title = "Count of New Construction Permits for the fishnet 2017",
        caption = "Figure 4.") +
   mapTheme()

 permit_count_map18 <- ggplot() +
   geom_sf(data = permit_net18, aes(fill = count_permits18)) +
   scale_fill_viridis() +
   labs(title = "Count of New Construction Permits for the fishnet 2018",
        caption = "Figure 5.") +
   mapTheme()

 permit_count_map19 <- ggplot() +
   geom_sf(data = permit_net19, aes(fill = count_permits19)) +
   scale_fill_viridis() +
   labs(title = "Count of New Construction Permits for the fishnet 2019",
        caption = "Figure 6.") +
   mapTheme()
 
 
permit_count_map14
permit_count_map15
permit_count_map16
permit_count_map17
permit_count_map18
permit_count_map19


```

```{r create census_net}
# function to make fishnet from acs dataframe (dat_tract), for variable (var_name), giving it the legend (legend_name), aggregating with function (sum_or_mean)
# for example, to make a fishnet summing all total_pop variables for 2013, one would define the variables as follows:
# dat_tract <- tracts13; var_name <- "total_pop"; legend_name <- "Total Population"; sum_or_mean <- sum
makenet <- function(dat_tract, var_name, legend_name, sum_or_mean){
  
  net_tract <- dat_tract %>% 
    .[philly,] %>% 
    dplyr::select(matches(var_name)) %>% 
    mutate(Legend = legend_name) %>% 
    st_join(fishnet, join=st_within) %>%
    st_drop_geometry() %>%
    group_by(uniqueID, Legend) %>%
    summarize(count = sum_or_mean(get(var_name), na.rm = T)) %>%
    left_join(fishnet, ., by = "uniqueID") %>%  # add geometry back in
    spread(Legend, count, fill=0) %>%  # fill in ones where fishnet was missing, count was NA with 0
    dplyr::select(-`<NA>`) %>%
    ungroup()
  
  return(net_tract)
}

var_names <- names(tracts13)[3:14]
legend_names <- c("Total Population","Median HH Income","Median Rent","Rent (as %age of Income)","mobility_tot_metro",
                  "samehouse1yr_metro","owner_occ","renter_occ","vehicles_avail","private_vehicle_occ","children","pct_white")
  
dat_tracts <- paste0("tracts",13:19)

# for loop to create fish nets for all ACS variables for all years
for (i in 1:length(dat_tracts)) {
  dat_tract <- get(dat_tracts[i])
  
  for (j in 1:length(var_names)){
    # select year, variable, and legend name
    yr <- i+12
    var_name <- var_names[j]
    legend_name <- paste(legend_names[j],yr)
    
    if (j %in% c(2:4,18)){
      sum_or_mean <- mean
    } else {
      sum_or_mean <- sum
    }
    
    # run function to make fishnet
    net_tract <- makenet(dat_tract,var_name,legend_name,sum_or_mean)
    
    # save as individual fishnet, uncomment if neededd
    # net_tract_name <- paste0("net_",var_name,yr)
    # assign(net_tract_name, net_tract)
    
    # save all variables into one big net
    if (i == 1 & j == 1) {
      census_net <- net_tract %>% 
        st_drop_geometry()
    } else {
      net_tract_tojoin <- net_tract %>% 
        st_drop_geometry() %>% 
        dplyr::select(1:2)
      census_net <- left_join(census_net, net_tract_tojoin, by = "uniqueID")
    }
  }
}


```


```{r create 311_net}
reports_15 <- reports311 %>% 
  filter(Year == 2015)
reports_16 <- reports311 %>% 
  filter(Year == 2016)
reports_17 <- reports311 %>% 
  filter(Year == 2017)
reports_18 <- reports311 %>% 
  filter(Year == 2018)
reports_19 <- reports311 %>% 
  filter(Year == 2019)

long_15 <- reports_15 %>% 
  arrange(type) %>%  
  mutate(Legend = paste(type, Year), .keep = "unused")
long_16 <- reports_16 %>% 
  arrange(type) %>%  
  mutate(Legend = paste(type, Year), .keep = "unused")
long_17 <- reports_17 %>% 
  arrange(type) %>%  
  mutate(Legend = paste(type, Year), .keep = "unused")
long_18 <- reports_18 %>% 
  arrange(type) %>%  
  mutate(Legend = paste(type, Year), .keep = "unused")
long_19 <- reports_19 %>% 
  arrange(type) %>%  
  mutate(Legend = paste(type, Year), .keep = "unused")

vars311_net <- rbind(long_15, long_16, long_17, long_18, long_19) %>% 
  st_join(fishnet, join = st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID, Legend) %>%
  summarize(count = n()) %>% 
  left_join(fishnet, ., by = "uniqueID") %>%  # add geometry back in
  spread(Legend, count, fill=0) %>%  # fill in ones where fishnet was missing, count was NA with 0
  dplyr::select(-`<NA>`) %>%
  ungroup()

# use something like code below (from lab 6) to create nearest neighbor counts
# convenience to reduce length of function names.
st_c    <- st_coordinates
st_coid <- st_centroid

vars311_net_ccoid <- st_c(st_coid(vars311_net))

# add nearest neighbor features
vars311_net <- vars311_net %>%
  mutate(
    buildingCon15.nn = nn_function(vars311_net_ccoid,st_c(long_15 %>% filter(Legend == "Building Construction 2015")), 3),
    dangerSide15.nn = nn_function(vars311_net_ccoid,st_c(long_15 %>% filter(Legend == "Dangerous Sidewalk 2015")), 3),
    illegalDump15.nn = nn_function(vars311_net_ccoid, st_c(long_15 %>% filter(Legend == "Illegal Dumping 2015")), 3),
    parksNrec15.nn = nn_function(
      vars311_net_ccoid,st_c(long_15 %>% filter(Legend == "Parks and Rec Safety and Maintenance 2015")), 3),
    materialColl15.nn = nn_function(
      vars311_net_ccoid,st_c(long_15 %>% filter(Legend == "Rubbish/Recyclable Material Collection 2015")), 3),
    streetTrees15.nn = nn_function(vars311_net_ccoid, st_c(long_15 %>% filter(Legend == "Street Trees 2015")), 3),
    vacant15.nn = nn_function(vars311_net_ccoid, st_c(long_15 %>% filter(Legend == "Vacant House or Commercial 2015")), 3),
             
    buildingCon16.nn = nn_function(vars311_net_ccoid,st_c(long_16 %>% filter(Legend == "Building Construction 2016")), 3),
    dangerSide16.nn = nn_function(vars311_net_ccoid,st_c(long_16 %>% filter(Legend == "Dangerous Sidewalk 2016")), 3),
    illegalDump16.nn = nn_function(vars311_net_ccoid, st_c(long_16 %>% filter(Legend == "Illegal Dumping 2016")), 3),
    parksNrec16.nn = nn_function(
      vars311_net_ccoid,st_c(long_16 %>% filter(Legend == "Parks and Rec Safety and Maintenance 2016")), 3),
    materialColl16.nn = nn_function(
      vars311_net_ccoid,st_c(long_16 %>% filter(Legend == "Rubbish/Recyclable Material Collection 2016")), 3),
    streetTrees16.nn = nn_function(vars311_net_ccoid, st_c(long_16 %>% filter(Legend == "Street Trees 2016")), 3),
    vacant16.nn = nn_function(vars311_net_ccoid, st_c(long_16 %>% filter(Legend == "Vacant House or Commercial 2016")), 3),
            
    buildingCon17.nn = nn_function(vars311_net_ccoid,st_c(long_17 %>% filter(Legend == "Building Construction 2017")), 3),
    dangerSide17.nn = nn_function(vars311_net_ccoid,st_c(long_17 %>% filter(Legend == "Dangerous Sidewalk 2017")), 3),
    illegalDump17.nn = nn_function(vars311_net_ccoid, st_c(long_17 %>% filter(Legend == "Illegal Dumping 2017")), 3),
    parksNrec17.nn = nn_function(
      vars311_net_ccoid,st_c(long_17 %>% filter(Legend == "Parks and Rec Safety and Maintenance 2017")), 3),
    materialColl17.nn = nn_function(
      vars311_net_ccoid,st_c(long_17 %>% filter(Legend == "Rubbish/Recyclable Material Collection 2017")), 3),
    streetTrees17.nn = nn_function(vars311_net_ccoid, st_c(long_17 %>% filter(Legend == "Street Trees 2017")), 3),
    vacant17.nn = nn_function(vars311_net_ccoid, st_c(long_17 %>% filter(Legend == "Vacant House or Commercial 2017")), 3),
         
    buildingCon18.nn = nn_function(vars311_net_ccoid,st_c(long_18 %>% filter(Legend == "Building Construction 2018")), 3),
    dangerSide18.nn = nn_function(vars311_net_ccoid,st_c(long_18 %>% filter(Legend == "Dangerous Sidewalk 2018")), 3),
    illegalDump18.nn = nn_function(vars311_net_ccoid, st_c(long_18 %>% filter(Legend == "Illegal Dumping 2018")), 3),
    parksNrec18.nn = nn_function(
      vars311_net_ccoid,st_c(long_18 %>% filter(Legend == "Parks and Rec Safety and Maintenance 2018")), 3),
    materialColl18.nn = nn_function(
      vars311_net_ccoid,st_c(long_18 %>% filter(Legend == "Rubbish/Recyclable Material Collection 2018")), 3),
    streetTrees18.nn = nn_function(vars311_net_ccoid, st_c(long_18 %>% filter(Legend == "Street Trees 2018")), 3),
    vacant18.nn = nn_function(vars311_net_ccoid, st_c(long_18 %>% filter(Legend == "Vacant House or Commercial 2018")), 3),
          
    buildingCon19.nn = nn_function(vars311_net_ccoid,st_c(long_19 %>% filter(Legend == "Building Construction 2019")), 3),
    dangerSide19.nn = nn_function(vars311_net_ccoid,st_c(long_19 %>% filter(Legend == "Dangerous Sidewalk 2019")), 3),
    illegalDump19.nn = nn_function(vars311_net_ccoid, st_c(long_19 %>% filter(Legend == "Illegal Dumping 2019")), 3),
    parksNrec19.nn = nn_function(
      vars311_net_ccoid,st_c(long_19 %>% filter(Legend == "Parks and Rec Safety and Maintenance 2019")), 3),
    materialColl19.nn = nn_function(
      vars311_net_ccoid,st_c(long_19 %>% filter(Legend == "Rubbish/Recyclable Material Collection 2019")), 3),
    streetTrees19.nn = nn_function(vars311_net_ccoid, st_c(long_19 %>% filter(Legend == "Street Trees 2019")), 3),
    vacant19.nn = nn_function(vars311_net_ccoid, st_c(long_19 %>% filter(Legend == "Vacant House or Commercial 2019")), 3),
    ) 
```


### Feature Selection


All census variables were pulled at the block group level, which is a smaller scale than our fishnet. There is no perfect way to join block-group-level census data to our fishnet, so our approach was to take the centroid of the block groups and aggregated the variables (by either taking the sum or average) of the block groups whose centroids landed in a fishnet grid cell (idk how to phrase this well so feel free to rephrase!). As a result, some fishnet cells that cover residential areas of Philadelphia may have a value of 0 for variables like "Total Population" since no block group centroid was within the bounds of that cell. 


```{r final_net}
# join census vars fishnet to permit count net + 311 net
all_net <- left_join(permit_net_allyrs, census_net, by = "uniqueID") %>% 
  left_join(vars311_net %>% st_drop_geometry(), by = "uniqueID") %>% 
  st_as_sf()

# make a subset of the net with variables we're actually interested in putting in the model
# add neighborhood names to data
# subset_net <- all_net %>%
#   st_centroid() %>%
#   st_join(dplyr::select(nhoods, mapname)) %>%
#   st_drop_geometry() %>%
#   left_join(dplyr::select(all_net, geometry, uniqueID), by = "uniqueID") %>%
#   st_sf() %>%
#   na.omit()

```

#### Figure 3. Correlation Matrix of all possible risk factors

```{r fig3/correlation matrix}
# making this moreso to see which would actually be good predictors
for_cormat <- all_net %>% 
  st_drop_geometry() %>% 
  dplyr::select(-c(uniqueID,cvID))

ggcorrplot(
  round(cor(for_cormat), 1), 
  # method = "circle",
  p.mat = cor_pmat(for_cormat),
  colors = c("#4b2875", "white", "#9c1339"),
  type="lower",
  insig = "blank",
  digits = 4,
  lab = T, lab_size = 2) +  
  labs(title = "Correlation",
       caption = "Figure x.") 

# strongest pred to keep: 
# 
```

### Spatial Process of Permit Count

Another interesting feature to add to the model is one that can help us define the spatial process of vandalism in Philadelphia. One tool to examine spatial process is local Moran's *I*, which helps us understand whether the vandalism count at a certain location is randomly distributed or clustered relative to its immediate neighbors.

### Models

Since the outcome we are interested in is a count or sum of vandalism incidents, I use a Poisson regression model to estimate the outcome. I created two types of models -- one with just risk factors as the predictors and another with both risk factors and spatial process (I ultimately only kept the model with both risk factors and spatial process).

### Validation

To assess the validity of my models I look at the accuracy (how accurate are my predictions in the set that the models are trained on?) and more importantly, generalizability (can I predict counts of vandalism across different neighborhoods? For another year?). 

1.  Accuracy

To assess accuracy of my models, I examined the mean absolute error (MAE; where error = predicted value - observed value) in the dataset containing vandalism incidents for 2021 -- the same data we used to train the model. 

2.  Generalizability

The most important validity measure of a geospatial risk prediction model is the generalizability as we want to be able to train a model and predict future outcomes, in our case counts of vandalism. I use random k-fold (with k ~ 100) cross validation as well as ‘Leave-one-group-out’ cross-validation (LOGO-CV) to assess model generalizability. LOGO-CV helps us assess model generalizability across neighborhoods. I also predict vandalism risk scores for the following year (2022) to assess whether the model generalizes across time using the 2021 kernel density.

## Results

### Visualizing Predictors and Local Moran's *I*

Sources of data:
median HH income for each year: https://www.deptofnumbers.com/income/pennsylvania/philadelphia/ 

[description of figure 4]

#### Figure 4. Predictors in final model

```{r fig4/small mult map, fig.height=15}
vars_net.long <- gather(subset_net %>% dplyr::select(-count_permits),
                        variable, value, -geometry, -uniqueID, -cvID, -mapname)

vars <- unique(vars_net.long$variable)
varList <- list()

for (i in vars) {
  varList[[i]] <- ggplot() +
      geom_sf(data = filter(vars_net.long, variable == i),aes(fill = value), colour=NA) +
      scale_fill_viridis(name = "") +
      labs(title = i) +
      mapTheme(title_size = 14) + theme(legend.position="bottom")
  }

do.call(grid.arrange,c(varList, ncol = 3, top = "Predictors of Permit Count (on fishnet)", bottom = "Figure x."))
```

[description of figure 5]

#### Figure 5. Adding indicators of significant spatial processes
```{r fig 5/small mult map local morans i, fig.height=10}
final_net.nb <- poly2nb(as_Spatial(subset_net), queen=TRUE)
final_net.weights <- nb2listw(final_net.nb, style="W", zero.policy=TRUE) # turn neighborhood weights into list of weights

local_morans <- localmoran(subset_net$count_permits, final_net.weights, zero.policy=TRUE) %>%
  as.data.frame() # Ii moran's I at ith cell, Ei expected/mean from neighbors

# join local Moran's I results to fishnet
final_net.localMorans <- 
  cbind(local_morans, as.data.frame(subset_net)) %>% 
  st_sf() %>%
  dplyr::select(`Permit Count` = count_permits, 
                `Local Morans I` = Ii, 
                `P Value` = `Pr(z != E(Ii))`) %>%
  mutate(`Significant Hotspots` = ifelse(`P Value` <= 0.001, 1, 0)) %>%
  gather(variable, value, -geometry)

# now plot
vars <- unique(final_net.localMorans$variable)
varList <- list()

for(i in vars){
  varList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(final_net.localMorans, variable == i),aes(fill = value), colour=NA) +
      scale_fill_viridis(name="") +
      labs(title=i) +
      mapTheme(title_size = 14) + theme(legend.position="bottom")
  }

do.call(grid.arrange,c(varList, ncol = 2, top = "Local Moran's I Statistics for Permit Count in Philadelphia", 
                       bottom = "Figure x."))

final_net <-
  subset_net %>% 
  mutate(permitct.isSig = 
           ifelse(localmoran(subset_net$count_permits, 
                             final_net.weights)[,5] <= 0.0000001, 1, 0)) %>%
  mutate(permitct.isSig.dist = 
           nn_function(st_coordinates(st_centroid(subset_net)),
                       st_coordinates(st_centroid(
                         filter(subset_net, permitct.isSig == 1))), 1))

```

[description of figure 6] 

#### Figure 6. Scatterplots of Predictors
```{r fig6/small mult scatter, fig.height=16}
correlation.long <-
  st_drop_geometry(final_net) %>%
    dplyr::select(-uniqueID, -cvID, -mapname) %>%
    gather(variable, value, -count_permits)

correlation.cor <-
  correlation.long %>%
    group_by(variable) %>%
    summarize(correlation = cor(value, count_permits, use = "complete.obs"))
    
ggplot(correlation.long, aes(value, count_permits)) +
  geom_point(size = 0.1) +
  geom_text(data = correlation.cor, aes(label = paste("r =", round(correlation, 2))),
            x=-Inf, y=Inf, vjust = 1.5, hjust = -.1) +
  geom_smooth(method = "lm", se = FALSE, colour = "black") +
  facet_wrap(~variable, ncol = 4, scales = "free") +
  labs(title = "Permit count as a function of predictors",
       caption = "Figure x.") +
  plotTheme(title_size = 14)
```

[description of figure 7]

#### Figure 7, Distribution of Vandalism count
```{r fig7/hist}
final_net %>% ggplot(aes(x = count_permits)) +
  geom_histogram(bins = 66) +
  theme_minimal() +
  labs(title = "Permit Distribution",
       x = "Count of New Construction Permits", y = "Count",
       caption = "Figure x.")
```

```{r models, results='hide'}
# just risk factors
reg.vars <- c("city_hall.nn", "in_bike_net", "is_historic", "ppr_sites.nn", "schools.nn", "septa_stops.nn", "total_hpss", "total_restaurants", "Trees", "Vacants")

## RUN REGRESSIONS
reg.CV <- crossValidate(
  dataset = final_net,
  id = "cvID",
  dependentVariable = "count_permits",
  indVariables = reg.vars) %>%
    mutate(error = count_permits - Prediction)

# MAE
reg.MAE <- mean(abs(reg.CV$error)) # 45.08406


# with local Moran's I spatial process features
reg.sp.vars <- c("city_hall.nn", "in_bike_net", "is_historic", "ppr_sites.nn", "schools.nn", "septa_stops.nn", "total_hpss", "total_restaurants", "Trees", "Vacants","permitct.isSig", "permitct.isSig.dist")

## RUN REGRESSIONS
reg.spatialCV <- crossValidate(
  dataset = final_net,
  id = "cvID",                           
  dependentVariable = "count_permits",
  indVariables = reg.sp.vars) %>% 
    dplyr::select(cvID, count_permits, Prediction, geometry) %>% 
    mutate(error = count_permits - Prediction)

# MAE
reg.spatial.MAE <- mean(abs(reg.spatialCV$error)) # 36.35607


# adding neighborhood for LOGO CV, risk factors only
reg.logoCV <- crossValidate(
  dataset = final_net,
  id = "mapname",
  dependentVariable = "count_permits",
  indVariables = reg.vars) %>%
    mutate(error = count_permits - Prediction)

# MAE
reg.logo.MAE <- mean(abs(reg.logoCV$error)) # 46.49673


# adding neighborhood for LOGO CV, risk factors + spatial process
reg.logo.spatialCV <- crossValidate(
  dataset = final_net,
  id = "mapname",                           
  dependentVariable = "count_permits",
  indVariables = reg.sp.vars) %>% 
    dplyr::select(cvID = mapname, count_permits, Prediction, geometry) %>% 
    mutate(error = count_permits - Prediction)

# MAE
reg.logo.spatial.MAE <- mean(abs(reg.logo.spatialCV$error)) # 37.12363

```

### Model Selection 



### Cross Validation

[description of figure 8 & 9]

#### Figure 8. Map of Model Errors
```{r fig8/small mult map errors}
reg.summary <- rbind(
  mutate(reg.spatialCV,
         Error = Prediction - count_permits,
         Regression = "Random k-fold CV"),
  mutate(reg.logo.spatialCV, 
         Error = Prediction - count_permits,
         Regression = "Spatial LOGO-CV")) %>%
    st_sf() 

error_by_reg_and_fold <- 
  reg.summary %>%
    group_by(Regression, cvID) %>% 
    summarize(Mean_Error = mean(Prediction - count_permits, na.rm = T),
              MAE        = mean(abs(Mean_Error), na.rm = T),
              SD_MAE     = mean(abs(Mean_Error), na.rm = T)) %>%
  ungroup()

# make map
error_by_reg_and_fold %>%
  ggplot() +
    geom_sf(aes(fill = MAE)) +
    facet_wrap(~Regression) +
    scale_fill_viridis() +
    labs(title = "Errors by Cross Validation method",
       caption = "Figure x.") +
    mapTheme(title_size = 14) + theme(legend.position="bottom")

```

#### Figure 9. Bar Plots of Error
```{r fig9/bar plots of MAE}
error_by_reg_and_fold %>%
  ggplot(aes(MAE)) + 
    geom_histogram(bins = 30, colour="black", fill = "#FDE725FF") +
    facet_wrap(~Regression) +  
    geom_vline(xintercept = 0) + scale_x_continuous(breaks = seq(0, 450, by = 50)) + 
    labs(title="Distribution of MAE", subtitle = "k-fold cross validation vs. LOGO-CV",
         x="Mean Absolute Error",     y="Count",
         caption = "Figure x.") +
    plotTheme(title_size = 14) + theme(legend.position="bottom")

```

[description of table 1]

#### Table 1. Summary of Regressions
```{r table1/MAE and SD}
st_drop_geometry(error_by_reg_and_fold) %>%
  group_by(Regression) %>% 
    summarize(`Mean MAE` = round(mean(MAE), 2),
              `SD MAE` = round(sd(MAE), 2)) %>%
  kable(caption = "Table 1: Summary of Regressions") %>%
  kable_styling("striped", full_width = F) %>% 
  kable_classic(full_width = F, html_font = "Cambria")

```

### Racial Context

In order to assess if the model generalizes to different neighborhood contexts, I specifically tested whether the model generalizes to a racial context. Using 2021 US Census data, I defined a neighborhood to be "Majority White" if over 50% of the population was White, and "Majority non-White" otherwise. 

[description of table 2]

#### Table 2. Racial Context
```{r table2/raw errors race2}
RaceContext <- tracts21 %>% 
  dplyr::select(GEOID,total_pop,RaceContext,geometry) %>% 
  .[nhoods,]

reg.summary %>% 
  st_centroid() %>%
  st_join(tracts21 %>% dplyr::select(RaceContext, geometry)) %>%
  na.omit() %>%
  st_drop_geometry() %>%
  group_by(Regression, RaceContext) %>%
  summarize(mean.Error = mean(Error, na.rm = T)) %>%
  spread(RaceContext, mean.Error) %>%
  kable(caption = "Table 2. Mean Error by Neighborhood Racial Context") %>%
  kable_styling("striped", full_width = F) %>% 
  kable_classic(html_font = "Cambria")

```

#### Table 2. Income Context

# need to work on this.
```{r table2/raw errors race}
IncomeContext <- tracts22 %>% 
  dplyr::select(GEOID,total_pop,IncomeContext,geometry) %>% 
  .[nhoods,]

reg.summary %>% 
  st_centroid() %>%
  st_join(tracts22 %>% dplyr::select(IncomeContext, geometry)) %>%
  na.omit() %>%
  st_drop_geometry() %>%
  group_by(Regression, IncomeContext) %>%
  summarize(mean.Error = mean(Error, na.rm = T)) %>%
  spread(IncomeContext, mean.Error) %>%
  kable(caption = "Table 2. Mean Error by Neighborhood Income Context") %>%
  kable_styling("striped", full_width = F) %>% 
  kable_classic(html_font = "Cambria")

```

### Kernel Density and Future Predictions

Finally, I examined the kernel density of 2021 vandalism counts in Philadelphia and tested whether this model generalizes to the vandalism incident report patterns of 2022. The risk categories, from 1st in purple to 5th in yellow are ordered from least to most risk of vandalism.

[description of figure 10]

#### Figure 10. Comparison of Kernel Density and Risk Predictions
```{r fig10/kernel density map, results='hide', fig.width=10, fig.height=10}
# kernel density
vand_ppp <- as.ppp(st_coordinates(vandalism), W = st_bbox(final_net))
vand_KD.1000 <- spatstat.explore::density.ppp(vand_ppp, 1000)
vand_KD.1500 <- spatstat.explore::density.ppp(vand_ppp, 1500)
vand_KD.2000 <- spatstat.explore::density.ppp(vand_ppp, 2000)
vand_KD.df <- rbind(
  mutate(data.frame(rasterToPoints(mask(raster(vand_KD.1000), as(nhoods, 'Spatial')))), Legend = "1000 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(vand_KD.1500), as(nhoods, 'Spatial')))), Legend = "1500 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(vand_KD.2000), as(nhoods, 'Spatial')))), Legend = "2000 Ft.")) 

vand_KD.df$Legend <- factor(vand_KD.df$Legend, levels = c("1000 Ft.", "1500 Ft.", "2000 Ft."))

# ggplot(data=vand_KD.df, aes(x=x, y=y)) +
#   geom_raster(aes(fill=layer)) + 
#   facet_wrap(~Legend) +
#   coord_sf(crs=st_crs(final_net)) + 
#   scale_fill_viridis(name="Density") +
#   labs(title = "Kernel density with 3 different search radii") +
#   mapTheme(title_size = 14)


# as.data.frame(vand_KD.1000) %>%
#   st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
#   aggregate(., final_net, mean) %>%
#    ggplot() +
#      geom_sf(aes(fill=value)) +
#      geom_sf(data = sample_n(vandalism, 1500), size = .5) +
#      scale_fill_viridis(name = "Density") +
#      labs(title = "Kernel density of 2021 Vandalism Incidents") +
#      mapTheme(title_size = 14)


# download data from 2022
# incidents22 <- st_read("~/Documents/MUSA5080/Assignments/HW04/incidents_22/incidents_part1_part2.shp") %>% 
#   st_transform('ESRI:102728') %>% rename(Legend = text_gener)
# 

carto_url = "https://phl.carto.com/api/v2/sql"

# Crime incidents
table_name = "incidents_part1_part2"

# query
where2 = "dispatch_date >= '2022-01-01' AND dispatch_date < '2023-01-01' AND text_general_code IN ('DRIVING UNDER THE INFLUENCE','Theft from Vehicle','Thefts','Disorderly Conduct','Public Drunkenness', 'Arson', 'Vandalism/Criminal Mischief')"

query2 = paste("SELECT *",
              "FROM", table_name,
              "WHERE", where)

incidents22 = rphl::get_carto(query2, format = "csv", base_url = carto_url, stringsAsFactors = F)%>%
   rename(Legend = text_general_code)
incidents22 <- incidents22 %>%
   filter(!is.na(point_x) & !is.na(point_y)) %>%
   st_as_sf(coords = c("point_x","point_y"),crs=4326) %>%
     st_transform('ESRI:102728') %>%
   dplyr::select(Legend, geometry)




# filter to only keep vandalism incidents
vandalism22 <- incidents22 %>% filter(Legend == "Vandalism/Criminal Mischief") %>% 
  .[fishnet,]


# from lab
vand_KDE_sum <- as.data.frame(vand_KD.1000) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) 
kde_breaks <- classIntervals(vand_KDE_sum$value, 
                             n = 5, "fisher")
vand_KDE_sf <- vand_KDE_sum %>%
  mutate(label = "Kernel Density",
         Risk_Category = classInt::findCols(kde_breaks),
         Risk_Category = case_when(
           Risk_Category == 5 ~ "5th",
           Risk_Category == 4 ~ "4th",
           Risk_Category == 3 ~ "3rd",
           Risk_Category == 2 ~ "2nd",
           Risk_Category == 1 ~ "1st")) %>%
  cbind(
    aggregate(
      dplyr::select(vandalism22) %>% mutate(vandCount = 1), ., sum) %>%
    mutate(vandCount = replace_na(vandCount, 0))) %>%
  dplyr::select(label, Risk_Category, vandCount)


ml_breaks <- classIntervals(reg.spatialCV$Prediction, 
                             n = 5, "fisher")
vand_risk_sf <-
  reg.spatialCV %>%
  mutate(label = "Risk Predictions",
         Risk_Category =classInt::findCols(ml_breaks),
         Risk_Category = case_when(
           Risk_Category == 5 ~ "5th",
           Risk_Category == 4 ~ "4th",
           Risk_Category == 3 ~ "3rd",
           Risk_Category == 2 ~ "2nd",
           Risk_Category == 1 ~ "1st")) %>%
  cbind(
    aggregate(
      dplyr::select(vandalism22) %>% mutate(vandCount = 1), ., sum) %>%
      mutate(vandCount = replace_na(vandCount, 0))) %>%
  dplyr::select(label,Risk_Category, vandCount)


rbind(vand_KDE_sf, vand_risk_sf) %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category, -geometry) %>%
  ggplot() +
    geom_sf(aes(fill = Risk_Category), colour = NA) +
    geom_sf(data = sample_n(vandalism22, 3000), size = .25, colour = "black") +
    facet_wrap(~label, ) +
    scale_fill_viridis(discrete = TRUE) +
    labs(title="Comparison of Kernel Density and Risk Predictions",
         subtitle="2021 Vandalism; 2022 Vandalism risk predictions",
         caption = "Figure 10.") +
    theme(legend.position="bottom") +
    mapTheme(title_size = 14)

```

[description of figure 11]

#### Figure 11.
```{r fig11/bar comparison}
rbind(vand_KDE_sf, vand_risk_sf) %>%
  st_drop_geometry() %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category) %>%
  group_by(label, Risk_Category) %>%
  summarize(countVand = sum(Value)) %>%
  ungroup() %>%
  group_by(label) %>%
  mutate(Pcnt_of_test_set_crimes = countVand / sum(countVand)) %>%
    ggplot(aes(Risk_Category,Pcnt_of_test_set_crimes)) +
      geom_bar(aes(fill=label), position="dodge", stat="identity") +
      scale_fill_viridis(discrete = TRUE, name = "Model") +
      labs(title = "Risk prediction vs. Kernel density, 2022 Vandalism Incidents",
           y = "% of Test Set Vandalism Incidents (per model)",
           x = "Risk Category",
           caption = "Figure 11.") +
  theme_bw() +
      theme(axis.text.x = element_text(angle = 45, vjust = 0.5))

```

## Conclusion

In conclusion....