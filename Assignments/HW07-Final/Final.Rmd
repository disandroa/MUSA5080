---
title: "Final"
author: "Akira Di Sandro"
date: "2023-11-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(sf)
library(tidyverse)
library(tidycensus)
library(viridis)
library(gridExtra)
library(ggpubr)
library(scales)
library(ggcorrplot)
library(spdep)
library(FNN)
library(kableExtra)
library(spatstat.explore)
library(raster)
library(classInt)

set.seed(172)

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

```

```{r data download + wrangling, results='hide'}
# philly bounds
philly <- st_read("https://opendata.arcgis.com/datasets/405ec3da942d4e20869d4e1449a2be48_0.geojson") %>%
  st_transform('ESRI:102728') %>% 
  dplyr::select(OBJECTID,geometry)

# read incidents file
incidents21 <- st_read("~/Documents/MUSA5080/Assignments/HW04/incidents_21/incidents_part1_part2.shp") %>% 
  st_transform('ESRI:102728') %>% rename(Legend = text_gener)

# filter to only keep vandalism incidents
vandalism <- incidents21 %>% filter(Legend == "Vandalism/Criminal Mischief") # 13670 rows

# only keep what's inside the philly boundary
vandalism <- vandalism[philly,] # 13578 rows

# same for other risk factors: public drunkenness, arson, disorderly conduct, thefts, theft from vehicle, dui
# also make feature for nearest neighbor count of these incidents
public_drunk <- incidents21 %>% filter(Legend == "Public Drunkenness") %>% 
  dplyr::select(Legend,geometry)
public_drunk <- public_drunk[philly,]

arson <- incidents21 %>% filter(Legend == "Arson") %>% 
  dplyr::select(Legend,geometry)
arson <- arson[philly,]

disorderly <- incidents21 %>% filter(Legend == "Disorderly Conduct") %>% 
  dplyr::select(Legend,geometry)
disorderly <- disorderly[philly,]

thefts <- incidents21 %>% filter(Legend == "Thefts") %>% 
  dplyr::select(Legend,geometry)
thefts <- thefts[philly,]

theft_from_veh <- incidents21 %>% filter(Legend == "Theft from Vehicle") %>% 
  dplyr::select(Legend,geometry)
theft_from_veh <- theft_from_veh[philly,]

dui <- incidents21 %>% filter(Legend == "DRIVING UNDER THE INFLUENCE") %>% 
  dplyr::select(Legend,geometry)
dui <- dui[philly,]

# add MedHHInc, TotalPop, maleFemaleRatio, popUnder18 (people under 25, can only find under 18 for now)
acs_vars <- c("B25026_001E","B01001A_001E","B19013_001E","B09001_001E","B01001_002E","B01001_026E", 
              "B01001_003E","B01001_004E","B01001_005E","B01001_006E","B01001_007E","B01001_008E","B01001_009E","B01001_010E",
              "B01001_027E","B01001_028E","B01001_029E","B01001_030E","B01001_031E","B01001_032E","B01001_033E","B01001_034E")

tract21 <- get_acs(geography = "tract", variables = acs_vars,
                   year = 2021, state = "PA", 
                   geometry = T, output = "wide") %>% 
  data.frame() %>% st_as_sf() %>% st_transform(st_crs(vandalism)) %>% 
  mutate(TotalPop = B25026_001E, 
         RaceContext = ifelse((B01001A_001E / TotalPop) > 0.5, "Majority White", "Majority non-White"),
         MedHHInc = B19013_001E,
         popUnder18 = B09001_001E,
         maleFemaleRatio = B01001_002E/B01001_026E,
         maleUnder25 = B01001_003E+B01001_004E+B01001_005E+B01001_006E+B01001_007E+B01001_008E+B01001_009E+B01001_010E,
         femaleUnder25 = B01001_027E+B01001_028E+B01001_029E+B01001_030E+B01001_031E+B01001_032E+B01001_033E+B01001_034E,
         popUnder25 = maleUnder25 + femaleUnder25) %>% 
  dplyr::select(GEOID, TotalPop:popUnder25, geometry)

# block group might be easier to split into fishnet
# blockgroup21 <- get_acs(geography = "block group", variables = acs_vars,
#                    year = 2021, state = "PA", 
#                    geometry = T, output = "wide") %>% 
#   data.frame() %>% 
#   mutate(TotalPop = B25026_001E, 
#          MedHHInc = B19013_001E,
#          popUnder18 = B09001_001E,
#          maleFemaleRatio = B01001_002E/B01001_026E,
#          maleUnder25 = B01001_003E+B01001_004E+B01001_005E+B01001_006E+B01001_007E+B01001_008E+B01001_009E+B01001_010E,
#          femaleUnder25 = B01001_027E+B01001_028E+B01001_029E+B01001_030E+B01001_031E+B01001_032E+B01001_033E+B01001_034E,
#          popUnder25 = maleUnder25 + femaleUnder25) %>% 
#   dplyr::select(GEOID, TotalPop:popUnder25, geometry)

# make sure Inf and NaN are turned into NA
tract21 <- tract21 %>% mutate(maleFemaleRatio = case_when(is.finite(maleFemaleRatio) ~ maleFemaleRatio,
                                                          is.infinite(maleFemaleRatio) ~ NA,
                                                          is.nan(maleFemaleRatio) ~ NA))

# separate into own factors
# totalpop <- tract21 %>% dplyr::select(TotalPop,geometry)
# totalpop <- totalpop[philly,]
# 
# medHHinc <- tract21 %>% dplyr::select(MedHHInc,geometry)
# medHHinc <- medHHinc[philly,]
# 
# under18 <- tract21 %>% dplyr::select(popUnder18,geometry)
# under18 <- under18[philly,]
# 
# mfratio <- tract21 %>% dplyr::select(maleFemaleRatio,geometry)
# mfratio <- mfratio[philly,]
# 
# munder25 <- tract21 %>% dplyr::select(maleUnder25,geometry)
# munder25 <- munder25[philly,]
# 
# funder25 <- tract21 %>% dplyr::select(femaleUnder25,geometry)
# funder25 <- funder25[philly,]
# 
# under25 <- tract21 %>% dplyr::select(popUnder25,geometry)
# under25 <- under25[philly,]

# for population of people under 25, might need to add
# sum B01001_003 - B01001_010 for males under 24
# sum B01001_027 - B01001_034 for females under 24

# how to join above spatial data to fishnet??


# philly neighborhoods
nhoods_path <- 'https://raw.githubusercontent.com/azavea/geo-data/master/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.geojson'
nhoods <- st_read(nhoods_path, quiet = T) %>%
  st_transform('ESRI:102728') %>%
  dplyr::select(mapname)

# police info
# police_districts <- st_read("https://opendata.arcgis.com/datasets/62ec63afb8824a15953399b1fa819df2_0.geojson") %>%
#   st_transform(st_crs(vandalism)) %>%
#   select(District = DISTRICT_)
# 
# police_service_areas <- st_read("https://opendata.arcgis.com/datasets/8dc58605f9dd484295c7d065694cdc0f_0.geojson") %>%
#   st_transform(st_crs(vandalism)) %>%
#   select(District = PSA_NUM)
# 
# bothPoliceUnits <- rbind(mutate(police_districts,     Legend = "Police Districts"), 
#                          mutate(police_service_areas, Legend = "Police Beats"))
 
# vacant lots/buildings
vacant_centroids = st_read("https://opendata.arcgis.com/datasets/b990222a527849229b4192feb4c42dc0_0.geojson") %>% 
            st_transform(st_crs(vandalism)) %>% 
            st_centroid() %>%
            mutate(Legend = "Vacants") %>%
            dplyr::select(Legend)

# parks & rec
ppr_sites = st_read("https://opendata.arcgis.com/api/v3/datasets/9eb26a787a6e448ba426eea7f9f0d93a_0/downloads/data?format=geojson&spatialRefId=4326") %>% st_transform(st_crs(vandalism)) %>% 
            mutate(Legend = "Parks and Rec") %>%
            dplyr::select(Legend)

# transit stops
el       <- st_read("https://opendata.arcgis.com/datasets/8c6e2575c8ad46eb887e6bb35825e1a6_0.geojson") %>% 
  st_transform('ESRI:102728')
bsl <- st_read("https://opendata.arcgis.com/datasets/2e9037fd5bef406488ffe5bb67d21312_0.geojson") %>% 
  st_transform('ESRI:102728')

septaStops <-
  rbind(
     el %>%
      mutate(Line = "El") %>%
      dplyr::select(Station, Line),
     bsl %>%
      mutate(Line ="Broad_St") %>%
      dplyr::select(Station, Line)) %>%
  st_transform(st_crs(vandalism)) %>%
  mutate(Legend = "Subway Stops") %>%
  dplyr::select(Legend)

# proximity to CBD using city_hall as proxy
city_hall <- bsl %>%
  st_transform(st_crs(vandalism)) %>%
  filter(Station=="City Hall") %>%
  mutate(Legend = "City Hall") %>%
  dplyr::select(Legend)

# schools
schools <- st_read('https://opendata.arcgis.com/datasets/d46a7e59e2c246c891fbee778759717e_0.geojson') %>% 
  st_transform('ESRI:102728') %>% 
  mutate(Legend = "Schools") %>% 
  dplyr::select(Legend)

# features of interest to add
# - density of property/buildings
# - proximity to CBD
# - proximity to transit
# - proximity to schools
# - proximity to parks and rec
# - distance to vacant lots

```

## Introduction

### Background: Vandalism in Philadelphia



#### Figure 1. Vandalism in Philadelphia (2021)

```{r fig1/outcome map}

vand_points <- ggplot() +
       geom_sf(data = philly, fill = "grey40") +
       geom_sf(data = vandalism, colour="#fde725ff", size=0.1, show.legend = "point") +
       labs(title= "Vandalism, Philadelphia - 2021") +
       mapTheme(title_size = 14)

vand_density <- ggplot() + 
       geom_sf(data = nhoods, fill = "grey40", color = "lightgrey") +
       stat_density2d(data = data.frame(st_coordinates(vandalism)),
                      aes(X, Y, fill = ..level.., alpha = ..level..),
                      size = 0.01, bins = 40, geom = 'polygon') +
       scale_fill_viridis() +
       scale_alpha(range = c(0.00, 0.35), guide = FALSE) +
       labs(title = "Density of Vandalism Incidents") +
       mapTheme(title_size = 14) + theme(legend.position = "none")

vand_list <- list(vand_points,vand_density)

do.call(grid.arrange, c(vand_list, ncol = 2, bottom = "Figure 1."))

```

## Methods

### The Fishnet



#### Figure 2. Vandalism on Fishnet

```{r fig2/fishnet outcome}
# our CRS is in feet, but want to make fishnet a 500m x 500m
cell_size <- 500 * 3.28084

fishnet <- 
  st_make_grid(philly,
               cellsize = cell_size,  
               square = TRUE) %>% 
  .[philly] %>%            
  st_sf() %>%              
  mutate(uniqueID = 1:n()) 

crime_net <- 
  dplyr::select(vandalism) %>% 
  mutate(countVand = 1) %>% 
  aggregate(., fishnet, sum) %>%
  mutate(countVand = replace_na(countVand, 0),
         uniqueID = as.numeric(rownames(.)),
         cvID = sample(round(nrow(fishnet) / 16), size=nrow(fishnet), replace = TRUE))

ggplot() +
  geom_sf(data = crime_net, aes(fill = countVand)) +
  scale_fill_viridis() +
  labs(title = "Count of Vandalism for the fishnet",
       caption = "Figure 2.") +
  mapTheme()

```

```{r create vars_net}
# net of variables of interest
vars_net <- rbind(public_drunk, arson, disorderly, thefts, theft_from_veh, dui,
                  # totalpop, medHHinc, under18, mfratio, munder25, funder25, under25, # don't know how to join these for now
                  vacant_centroids, ppr_sites, septaStops, city_hall, schools) %>%
  st_join(fishnet, join=st_within) %>%  # if the point from abandonCars is in the fishnet, assign the unique ID to that point
  st_drop_geometry() %>%
  group_by(uniqueID, Legend) %>%
  summarize(count = n()) %>%
  left_join(fishnet, ., by = "uniqueID") %>%  # add geometry back in
  spread(Legend, count, fill=0) %>%  # fill in ones where fishnet was missing, count was NA with 0
  dplyr::select(-`<NA>`) %>%
  ungroup()


# use something like code below (from lab 6) to create nearest neighbor counts
# convenience to reduce length of function names.
st_c    <- st_coordinates
st_coid <- st_centroid

vars_net_ccoid <- st_c(st_coid(vars_net))

## create NN from abandoned cars
vars_net <- vars_net %>%
    mutate(public_drunk.nn = nn_function(vars_net_ccoid, st_c(public_drunk), 8),
           arson.nn = nn_function(vars_net_ccoid, st_c(arson), 8),
           disorderly.nn = nn_function(vars_net_ccoid, st_c(disorderly), 8),
           thefts.nn = nn_function(vars_net_ccoid, st_c(thefts), 8),
           theft_from_veh.nn = nn_function(vars_net_ccoid, st_c(theft_from_veh), 8),
           dui.nn = nn_function(vars_net_ccoid, st_c(dui), 8),
           vacant_centroids.nn = nn_function(vars_net_ccoid, st_c(vacant_centroids), 8),
           ppr_sites.nn = nn_function(vars_net_ccoid, st_c(ppr_sites), 3),
           septa_stops.nn = nn_function(vars_net_ccoid, st_c(septaStops), 2),
           city_hall.nn = nn_function(vars_net_ccoid, st_c(city_hall), 1),
           schools.nn = nn_function(vars_net_ccoid, st_c(schools), 8)) 

```

### Risk Factor Selection



```{r final_net}
all_net <-
  left_join(crime_net, st_drop_geometry(vars_net), by="uniqueID") 

# might want to change above depending on model
# trying things that stood out from model
# final_net1 <- all_net %>% 
#   dplyr::select(countVand,uniqueID,cvID,Arson,`Theft from Vehicle`,Vacants,dui.nn,
#                 schools.nn,septa_stops.nn,ppr_sites.nn,geometry) %>% 
#   rename(`Vacant Lots/Buildings` = Vacants, 
#          `DUI (nn)` = dui.nn,
#          `Schools (nn)` = schools.nn,
#          `Subway Stops (nn)` = septa_stops.nn,
#          `Parks and Rec (nn)` = ppr_sites.nn)

# subset_net <- all_net %>% 
#   dplyr::select(countVand,uniqueID,cvID,Arson,`Theft from Vehicle`,Vacants,dui.nn) %>% 
#   rename(Thefts_from_Vehicle = `Theft from Vehicle`,
#          Vacant_lots_buildings = Vacants,
#          DUI.nn = dui.nn) %>% 
#   st_centroid() %>%
#   st_join(dplyr::select(nhoods, mapname)) %>%
#   st_drop_geometry() %>%
#   left_join(dplyr::select(all_net, geometry, uniqueID), by = "uniqueID") %>%
#   st_sf() %>%
#   na.omit()

subset_net <- all_net %>% 
  dplyr::select(countVand,uniqueID,cvID,Arson,`Theft from Vehicle`,vacant_centroids.nn,dui.nn,
                thefts.nn,disorderly.nn) %>% 
  rename(Thefts_from_Vehicle = `Theft from Vehicle`,
         vacant_lots_buildings.nn = vacant_centroids.nn,
         DUI.nn = dui.nn,
         Thefts.nn = thefts.nn,
         Disorderly_conduct.nn = disorderly.nn) %>% 
  st_centroid() %>%
  st_join(dplyr::select(nhoods, mapname)) %>%
  st_drop_geometry() %>%
  left_join(dplyr::select(all_net, geometry, uniqueID), by = "uniqueID") %>%
  st_sf() %>%
  na.omit()

```

#### Figure 3. Correlation Matrix of all possible risk factors

```{r fig3/correlation matrix}
# making this moreso to see which would actually be good predictors
for_cormat <- all_net %>% st_drop_geometry() %>% 
  # dplyr::select(countVand,Arson,arson.nn,`City Hall`,city_hall.nn,`Disorderly Conduct`,disorderly.nn, 
  #               `DRIVING UNDER THE INFLUENCE`,dui.nn,`Parks and Rec`,ppr_sites.nn,`Public Drunkenness`,
  #               public_drunk.nn,Schools,schools.nn,`Subway Stops`,septa_stops.nn,`Theft from Vehicle`,
  #               theft_from_veh.nn,Thefts,thefts.nn,Vacants,vacant_centroids.nn)
  dplyr::select(-c(uniqueID,cvID)) %>% 
  rename(DUI = `DRIVING UNDER THE INFLUENCE`)

ggcorrplot(
  round(cor(for_cormat), 1), 
  # method = "circle",
  p.mat = cor_pmat(for_cormat),
  colors = c("#4b2875", "white", "#9c1339"),
  type="lower",
  insig = "blank",
  digits = 4,
  lab = T, lab_size = 2) +  
    labs(title = "Correlation",
         caption = "Figure 3.") 

# strong pred to keep: 
# arson/arson.nn, theft from veh, vacants/vacant.nn, dui.nn
# schools.nn, septa_stops.nn, ppr.nn, thefts/thefts.nn, disorderly.nn, 

```

### Spatial Process of Vandalism

Another interesting feature to add to the model is one that can help us define the spatial process of vandalism in Philadelphia. One tool to examine spatial process is local Moran's *I*, which helps us understand whether the vandalism count at a certain location is randomly distributed or clustered relative to its immediate neighbors.

### Models

Since the outcome we are interested in is a count or sum of vandalism incidents, I use a Poisson regression model to estimate the outcome. I created two types of models -- one with just risk factors as the predictors and another with both risk factors and spatial process (I ultimately only kept the model with both risk factors and spatial process).

### Validation

To assess the validity of my models I look at the accuracy (how accurate are my predictions in the set that the models are trained on?) and more importantly, generalizability (can I predict counts of vandalism across different neighborhoods? For another year?). 

1.  Accuracy

To assess accuracy of my models, I examined the mean absolute error (MAE; where error = predicted value - observed value) in the dataset containing vandalism incidents for 2021 -- the same data we used to train the model. 

2.  Generalizability

The most important validity measure of a geospatial risk prediction model is the generalizability as we want to be able to train a model and predict future outcomes, in our case counts of vandalism. I use random k-fold (with k ~ 100) cross validation as well as ‘Leave-one-group-out’ cross-validation (LOGO-CV) to assess model generalizability. LOGO-CV helps us assess model generalizability across neighborhoods. I also predict vandalism risk scores for the following year (2022) to assess whether the model generalizes across time using the 2021 kernel density.

## Results

### Visualizing Risk factors and Local Moran's *I*

[description of figure 4]

#### Figure 4. Risk Factors in final model

```{r fig4/small mult map, fig.height=15}
vars_net.long <- gather(subset_net %>% dplyr::select(-countVand),
                        variable, value, -geometry, -uniqueID, -cvID, -mapname)

vars <- unique(vars_net.long$variable)
varList <- list()

for(i in vars){
  varList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(vars_net.long, variable == i),aes(fill = value), colour=NA) +
      scale_fill_viridis(name="") +
      labs(title=i) +
      mapTheme(title_size = 14) + theme(legend.position="bottom")}

do.call(grid.arrange,c(varList, ncol = 2, top = "Risk Factors for Vandalism (on fishnet)", bottom = "Figure 4."))
```

[description of figure 5]

#### Figure 5. Adding indicators of significant spatial processes
```{r fig 5/small mult map local morans i, fig.height=10}
final_net.nb <- poly2nb(as_Spatial(subset_net), queen=TRUE)
final_net.weights <- nb2listw(final_net.nb, style="W", zero.policy=TRUE) # turn neighborhood weights into list of weights


local_morans <- localmoran(subset_net$countVand, final_net.weights, zero.policy=TRUE) %>%
  as.data.frame() # Ii moran's I at ith cell, Ei expected/mean from neighbors

# join local Moran's I results to fishnet
final_net.localMorans <- 
  cbind(local_morans, as.data.frame(subset_net)) %>% 
  st_sf() %>%
  dplyr::select(`Vandalism Count` = countVand, 
                `Local Morans I` = Ii, 
                `P Value` = `Pr(z != E(Ii))`) %>%
  mutate(`Significant Hotspots` = ifelse(`P Value` <= 0.001, 1, 0)) %>%
  gather(variable, value, -geometry)

# now plot
vars <- unique(final_net.localMorans$variable)
varList <- list()

for(i in vars){
  varList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(final_net.localMorans, variable == i),aes(fill = value), colour=NA) +
      scale_fill_viridis(name="") +
      labs(title=i) +
      mapTheme(title_size = 14) + theme(legend.position="bottom")}

do.call(grid.arrange,c(varList, ncol = 2, top = "Local Moran's I Statistics for Vandalism in Philadelphia", 
                       bottom = "Figure 5."))

final_net <-
  subset_net %>% 
  mutate(vandalism.isSig = 
           ifelse(localmoran(subset_net$countVand, 
                             final_net.weights)[,5] <= 0.0000001, 1, 0)) %>%
  mutate(vandalism.isSig.dist = 
           nn_function(st_coordinates(st_centroid(subset_net)),
                       st_coordinates(st_centroid(
                         filter(subset_net, vandalism.isSig == 1))), 1))

```

[description of figure 6] 

#### Figure 6. Scatterplots of Risk Factors
```{r fig6/small mult scatter, fig.height=16}
# from book

correlation.long <-
  st_drop_geometry(final_net) %>%
    dplyr::select(-uniqueID, -cvID, -mapname) %>%
    gather(variable, value, -countVand)

correlation.cor <-
  correlation.long %>%
    group_by(variable) %>%
    summarize(correlation = cor(value, countVand, use = "complete.obs"))
    
ggplot(correlation.long, aes(value, countVand)) +
  geom_point(size = 0.1) +
  geom_text(data = correlation.cor, aes(label = paste("r =", round(correlation, 2))),
            x=-Inf, y=Inf, vjust = 1.5, hjust = -.1) +
  geom_smooth(method = "lm", se = FALSE, colour = "black") +
  facet_wrap(~variable, ncol = 2, scales = "free") +
  labs(title = "Vandalism count as a function of risk factors",
       caption = "Figure 6.") +
  plotTheme(title_size = 14)
```

[description of figure 7]

#### Figure 7, Distribution of Vandalism count
```{r fig7/hist}
final_net %>% ggplot(aes(x = countVand)) +
  geom_histogram(bins = 66) +
  theme_minimal() +
  labs(title = "Vandalism Distribution",
       x = "Incidents of Vandalism", y = "Count",
       caption = "Figure 7.")
```

```{r models, results='hide'}
# just risk factors
# reg.vars <- c("Arson", "Thefts_from_Vehicle", "vacant_lots_buildings.nn", "DUI.nn",
#               "Thefts.nn", "Disorderly_conduct.nn")
# 
# ## RUN REGRESSIONS
# reg.CV <- crossValidate(
#   dataset = final_net,
#   id = "cvID",                           
#   dependentVariable = "countVand",
#   indVariables = reg.vars) %>% 
#     mutate(error = countVand - Prediction)
# 
# # MAE
# reg.MAE <- mean(abs(reg.CV$error)) # 3.730237


# with local Moran's I spatial process features
reg.sp.vars <- c("Arson", "Thefts_from_Vehicle", "vacant_lots_buildings.nn", "DUI.nn",
                 "Thefts.nn", "Disorderly_conduct.nn","vandalism.isSig", "vandalism.isSig.dist")

## RUN REGRESSIONS
reg.spatialCV <- crossValidate(
  dataset = final_net,
  id = "cvID",                           
  dependentVariable = "countVand",
  indVariables = reg.sp.vars) %>% 
    dplyr::select(cvID, countVand, Prediction, geometry)
    # mutate(error = countVand - Prediction)

# MAE
# reg.spatial.MAE <- mean(abs(reg.spatialCV$error)) # 3.63178 


# adding neighborhood for LOGO CV, risk factors only
# reg.logoCV <- crossValidate(
#   dataset = final_net,
#   id = "mapname",                           
#   dependentVariable = "countVand",
#   indVariables = reg.vars) %>% 
#     mutate(error = countVand - Prediction)
# 
# # MAE
# reg.logo.MAE <- mean(abs(reg.logoCV$error)) # 3.742847


# adding neighborhood for LOGO CV, risk factors + spatial process
reg.logo.spatialCV <- crossValidate(
  dataset = final_net,
  id = "mapname",                           
  dependentVariable = "countVand",
  indVariables = reg.sp.vars) %>% 
    dplyr::select(cvID = mapname, countVand, Prediction, geometry)
    # mutate(error = countVand - Prediction)

# MAE
# reg.logo.spatial.MAE <- mean(abs(reg.logo.spatialCV$error)) # 3.651649


```

### Model Selection 

As mentioned in the Methods section, I kept both risk factors and spatial process as independent variables in my model as this model had lower values of error across all cross validation methods.

### Cross Validation

[description of figure 8 & 9]

#### Figure 8. Map of Model Errors
```{r fig8/small mult map errors}
reg.summary <- rbind(
  mutate(reg.spatialCV,
         Error = Prediction - countVand,
         Regression = "Random k-fold CV"),
  mutate(reg.logo.spatialCV, 
         Error = Prediction - countVand,
         Regression = "Spatial LOGO-CV")) %>%
    st_sf() 

error_by_reg_and_fold <- 
  reg.summary %>%
    group_by(Regression, cvID) %>% 
    summarize(Mean_Error = mean(Prediction - countVand, na.rm = T),
              MAE        = mean(abs(Mean_Error), na.rm = T),
              SD_MAE     = mean(abs(Mean_Error), na.rm = T)) %>%
  ungroup()

# make map
error_by_reg_and_fold %>%
  ggplot() +
    geom_sf(aes(fill = MAE)) +
    facet_wrap(~Regression) +
    scale_fill_viridis() +
    labs(title = "Errors by Cross Validation method",
       caption = "Figure 8.") +
    mapTheme(title_size = 14) + theme(legend.position="bottom")

```

#### Figure 9. Bar Plots of Error
```{r fig9/bar plots of MAE}
error_by_reg_and_fold %>%
  ggplot(aes(MAE)) + 
    geom_histogram(bins = 30, colour="black", fill = "#FDE725FF") +
    facet_wrap(~Regression) +  
    geom_vline(xintercept = 0) + scale_x_continuous(breaks = seq(0, 30, by = 2)) + 
    labs(title="Distribution of MAE", subtitle = "k-fold cross validation vs. LOGO-CV",
         x="Mean Absolute Error",     y="Count",
         caption = "Figure 9.") +
    plotTheme(title_size = 14) + theme(legend.position="bottom")

```

[description of table 1]

#### Table 1. Summary of Regressions
```{r table1/MAE and SD}
st_drop_geometry(error_by_reg_and_fold) %>%
  group_by(Regression) %>% 
    summarize(`Mean MAE` = round(mean(MAE), 2),
              `SD MAE` = round(sd(MAE), 2)) %>%
  kable(caption = "Table 1: Summary of Regressions") %>%
  kable_styling("striped", full_width = F) %>% 
  kable_classic(full_width = F, html_font = "Cambria")

```

### Racial Context

In order to assess if the model generalizes to different neighborhood contexts, I specifically tested whether the model generalizes to a racial context. Using 2021 US Census data, I defined a neighborhood to be "Majority White" if over 50% of the population was White, and "Majority non-White" otherwise. 

[description of table 2]

#### Table 2. Racial Context
```{r table2/raw errors race}
RaceContext <- tract21 %>% dplyr::select(GEOID,TotalPop,RaceContext,geometry) %>% .[nhoods,]

reg.summary %>% 
  st_centroid() %>%
  st_join(tract21 %>% dplyr::select(RaceContext, geometry)) %>%
  na.omit() %>%
  st_drop_geometry() %>%
  group_by(Regression, RaceContext) %>%
  summarize(mean.Error = mean(Error, na.rm = T)) %>%
  spread(RaceContext, mean.Error) %>%
  kable(caption = "Table 2. Mean Error by Neighborhood Racial Context") %>%
  kable_styling("striped", full_width = F) %>% 
  kable_classic(html_font = "Cambria")

```

### Kernel Density and Future Predictions

Finally, I examined the kernel density of 2021 vandalism counts in Philadelphia and tested whether this model generalizes to the vandalism incident report patterns of 2022. The risk categories, from 1st in purple to 5th in yellow are ordered from least to most risk of vandalism.

[description of figure 10]

#### Figure 10. Comparison of Kernel Density and Risk Predictions
```{r fig10/kernel density map, results='hide', fig.width=10, fig.height=10}
# kernel density
vand_ppp <- as.ppp(st_coordinates(vandalism), W = st_bbox(final_net))
vand_KD.1000 <- spatstat.explore::density.ppp(vand_ppp, 1000)
vand_KD.1500 <- spatstat.explore::density.ppp(vand_ppp, 1500)
vand_KD.2000 <- spatstat.explore::density.ppp(vand_ppp, 2000)
vand_KD.df <- rbind(
  mutate(data.frame(rasterToPoints(mask(raster(vand_KD.1000), as(nhoods, 'Spatial')))), Legend = "1000 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(vand_KD.1500), as(nhoods, 'Spatial')))), Legend = "1500 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(vand_KD.2000), as(nhoods, 'Spatial')))), Legend = "2000 Ft.")) 

vand_KD.df$Legend <- factor(vand_KD.df$Legend, levels = c("1000 Ft.", "1500 Ft.", "2000 Ft."))

# ggplot(data=vand_KD.df, aes(x=x, y=y)) +
#   geom_raster(aes(fill=layer)) + 
#   facet_wrap(~Legend) +
#   coord_sf(crs=st_crs(final_net)) + 
#   scale_fill_viridis(name="Density") +
#   labs(title = "Kernel density with 3 different search radii") +
#   mapTheme(title_size = 14)


# as.data.frame(vand_KD.1000) %>%
#   st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
#   aggregate(., final_net, mean) %>%
#    ggplot() +
#      geom_sf(aes(fill=value)) +
#      geom_sf(data = sample_n(vandalism, 1500), size = .5) +
#      scale_fill_viridis(name = "Density") +
#      labs(title = "Kernel density of 2021 Vandalism Incidents") +
#      mapTheme(title_size = 14)


# download data from 2022
incidents22 <- st_read("~/Documents/MUSA5080/Assignments/HW04/incidents_22/incidents_part1_part2.shp") %>% 
  st_transform('ESRI:102728') %>% rename(Legend = text_gener)

# filter to only keep vandalism incidents
vandalism22 <- incidents21 %>% filter(Legend == "Vandalism/Criminal Mischief") %>% 
  .[fishnet,]


# from lab
vand_KDE_sum <- as.data.frame(vand_KD.1000) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) 
kde_breaks <- classIntervals(vand_KDE_sum$value, 
                             n = 5, "fisher")
vand_KDE_sf <- vand_KDE_sum %>%
  mutate(label = "Kernel Density",
         Risk_Category = classInt::findCols(kde_breaks),
         Risk_Category = case_when(
           Risk_Category == 5 ~ "5th",
           Risk_Category == 4 ~ "4th",
           Risk_Category == 3 ~ "3rd",
           Risk_Category == 2 ~ "2nd",
           Risk_Category == 1 ~ "1st")) %>%
  cbind(
    aggregate(
      dplyr::select(vandalism22) %>% mutate(vandCount = 1), ., sum) %>%
    mutate(vandCount = replace_na(vandCount, 0))) %>%
  dplyr::select(label, Risk_Category, vandCount)


ml_breaks <- classIntervals(reg.spatialCV$Prediction, 
                             n = 5, "fisher")
vand_risk_sf <-
  reg.spatialCV %>%
  mutate(label = "Risk Predictions",
         Risk_Category =classInt::findCols(ml_breaks),
         Risk_Category = case_when(
           Risk_Category == 5 ~ "5th",
           Risk_Category == 4 ~ "4th",
           Risk_Category == 3 ~ "3rd",
           Risk_Category == 2 ~ "2nd",
           Risk_Category == 1 ~ "1st")) %>%
  cbind(
    aggregate(
      dplyr::select(vandalism22) %>% mutate(vandCount = 1), ., sum) %>%
      mutate(vandCount = replace_na(vandCount, 0))) %>%
  dplyr::select(label,Risk_Category, vandCount)


rbind(vand_KDE_sf, vand_risk_sf) %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category, -geometry) %>%
  ggplot() +
    geom_sf(aes(fill = Risk_Category), colour = NA) +
    geom_sf(data = sample_n(vandalism22, 3000), size = .25, colour = "black") +
    facet_wrap(~label, ) +
    scale_fill_viridis(discrete = TRUE) +
    labs(title="Comparison of Kernel Density and Risk Predictions",
         subtitle="2021 Vandalism; 2022 Vandalism risk predictions",
         caption = "Figure 10.") +
    theme(legend.position="bottom") +
    mapTheme(title_size = 14)

```

[description of figure 11]

#### Figure 11.
```{r fig11/bar comparison}
rbind(vand_KDE_sf, vand_risk_sf) %>%
  st_drop_geometry() %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category) %>%
  group_by(label, Risk_Category) %>%
  summarize(countVand = sum(Value)) %>%
  ungroup() %>%
  group_by(label) %>%
  mutate(Pcnt_of_test_set_crimes = countVand / sum(countVand)) %>%
    ggplot(aes(Risk_Category,Pcnt_of_test_set_crimes)) +
      geom_bar(aes(fill=label), position="dodge", stat="identity") +
      scale_fill_viridis(discrete = TRUE, name = "Model") +
      labs(title = "Risk prediction vs. Kernel density, 2022 Vandalism Incidents",
           y = "% of Test Set Vandalism Incidents (per model)",
           x = "Risk Category",
           caption = "Figure 11.") +
  theme_bw() +
      theme(axis.text.x = element_text(angle = 45, vjust = 0.5))

```

## Conclusion

In conclusion....